{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bc0ed2-c724-4a0b-94a5-90ed60ba7cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chath/nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Update the vocabulary with words from each review\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 51\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(words)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Print the most common words\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chath/nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9f1156-8549-4232-b230-e39ea729fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\ml_first_project\\env\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in d:\\ml_first_project\\env\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\ml_first_project\\env\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\ml_first_project\\env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\ml_first_project\\env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\ml_first_project\\env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df32c60-ea8c-410c-9de0-c3a51a7af9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chath/nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Update the vocabulary with words from each review\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 51\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(words)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Print the most common words\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mD:\\ML_first_project\\env\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chath/nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\ML_first_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chath\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "326f8451-b57e-4858-8b99-9c0ea0bb881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ce3ea0-e0dd-4843-82e9-e4740eaf3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30f4299-fe5a-4996-8449-db735709d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words:\n",
      "[('physiotherapist', 110), ('care', 47), ('patient', 46), ('time', 44), ('medic', 43), ('recommend', 33), ('test', 33), ('good', 31), ('explain', 30), (\"n't\", 29)]\n",
      "   Unnamed: 0                                            reviews  labels  \\\n",
      "0          93  he explained initially that it takes 4-5 sitin...       0   \n",
      "1          33  great dr definitely recommend he recommends le...       1   \n",
      "2         129  doctor came and spent 9 seconds and recommende...       0   \n",
      "3           7  i am completely satisfied with the consultatio...       1   \n",
      "4          38  my experience was nice dr dyed was cool and co...       1   \n",
      "\n",
      "        tag                                  processed_reviews  \\\n",
      "0  negative  explain initi take - site total care cost rupe...   \n",
      "1  positive  great dr definit recommend recommend less medi...   \n",
      "2  negative  physiotherapist came spent second recommend no...   \n",
      "3  positive  complet satisfi telemedicin consult acut sever...   \n",
      "4  positive  experi nice dr dy cool compos patient listen m...   \n",
      "\n",
      "                               binary_representation  \n",
      "0  {'explain': 1, 'initi': 1, 'take': 1, '-': 1, ...  \n",
      "1  {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
      "2  {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n",
      "3  {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
      "4  {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize objects\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Replace words dictionary (example - you can customize this dictionary)\n",
    "word_replacement = {\n",
    "    'doctor': 'physiotherapist',\n",
    "    'treatment': 'care',\n",
    "    'medicine': 'medication',\n",
    "    'expensive': 'high cost',\n",
    "    'physiotherapist': 'physical therapist',\n",
    "    'consultation': 'telemedicine consultation',\n",
    "    'healthcare': 'telemedicine services',\n",
    "    'clinic': 'online platform'\n",
    "}\n",
    "\n",
    "dataset = pd.read_csv('D:/ML_first_project/artifacts/doctorReviews (2).csv')\n",
    "\n",
    "# Function for preprocessing\n",
    "def preprocess(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # Replace words\n",
    "    text = ' '.join([word_replacement.get(word, word) for word in text.split()])\n",
    "\n",
    "    # Tokenization and Stemming\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to the reviews column\n",
    "dataset[\"processed_reviews\"] = dataset[\"reviews\"].apply(preprocess)\n",
    "\n",
    "# Build vocabulary using Counter\n",
    "vocab = Counter()\n",
    "\n",
    "# Update the vocabulary with words from each review\n",
    "for sentence in dataset[\"processed_reviews\"]:\n",
    "    \n",
    "    vocab.update(sentence.split())\n",
    "\n",
    "# Print the most common words\n",
    "print(\"Most Common Words:\")\n",
    "print(vocab.most_common(10))\n",
    "\n",
    "# Convert vocabulary to binary (1 if the word exists in the review, else 0)\n",
    "def convert_to_binary(vocab, text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    binary_rep = {word: 1 if word in words else 0 for word in vocab}\n",
    "    return binary_rep\n",
    "\n",
    "# Example of converting one review into binary representation\n",
    "binary_representation = dataset[\"processed_reviews\"].apply(lambda x: convert_to_binary(vocab, x))\n",
    "\n",
    "# Add binary representation to the dataset (as a new column)\n",
    "dataset[\"binary_representation\"] = binary_representation\n",
    "\n",
    "# Print a sample from the dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd844f5-8218-4c4a-81ff-dff03348c14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'physiotherapist': 110,\n",
       "         'care': 47,\n",
       "         'patient': 46,\n",
       "         'time': 44,\n",
       "         'medic': 43,\n",
       "         'recommend': 33,\n",
       "         'test': 33,\n",
       "         'good': 31,\n",
       "         'explain': 30,\n",
       "         \"n't\": 29,\n",
       "         'problem': 28,\n",
       "         'visit': 26,\n",
       "         'appoint': 23,\n",
       "         'hospit': 22,\n",
       "         'experi': 21,\n",
       "         'dr': 20,\n",
       "         'friendli': 19,\n",
       "         'issu': 19,\n",
       "         'wait': 19,\n",
       "         'consult': 18,\n",
       "         'day': 17,\n",
       "         'â€™': 17,\n",
       "         'said': 17,\n",
       "         'ask': 17,\n",
       "         'satisfi': 16,\n",
       "         'health': 16,\n",
       "         'get': 16,\n",
       "         'suggest': 16,\n",
       "         'like': 15,\n",
       "         'gave': 15,\n",
       "         'happi': 15,\n",
       "         \"'s\": 14,\n",
       "         'need': 14,\n",
       "         'give': 14,\n",
       "         'even': 14,\n",
       "         'check': 14,\n",
       "         'take': 13,\n",
       "         'listen': 13,\n",
       "         'go': 12,\n",
       "         'explan': 12,\n",
       "         'way': 12,\n",
       "         'onlin': 12,\n",
       "         'went': 12,\n",
       "         'peopl': 11,\n",
       "         'avail': 11,\n",
       "         'first': 11,\n",
       "         'nice': 10,\n",
       "         'anoth': 10,\n",
       "         'treat': 10,\n",
       "         'money': 10,\n",
       "         'mani': 10,\n",
       "         'platform': 10,\n",
       "         'took': 10,\n",
       "         'realli': 10,\n",
       "         '&': 9,\n",
       "         'one': 9,\n",
       "         'well': 9,\n",
       "         'apollo': 9,\n",
       "         'result': 9,\n",
       "         'also': 9,\n",
       "         'doctor': 9,\n",
       "         'medicin': 9,\n",
       "         'would': 9,\n",
       "         'hour': 9,\n",
       "         'prescrib': 9,\n",
       "         'cost': 8,\n",
       "         'see': 8,\n",
       "         'talk': 8,\n",
       "         'us': 8,\n",
       "         'say': 8,\n",
       "         'minut': 8,\n",
       "         'done': 8,\n",
       "         'call': 8,\n",
       "         'much': 8,\n",
       "         'endoscopi': 7,\n",
       "         'telemedicin': 7,\n",
       "         'know': 7,\n",
       "         'next': 7,\n",
       "         'hope': 7,\n",
       "         'experienc': 7,\n",
       "         'work': 7,\n",
       "         'use': 7,\n",
       "         'never': 7,\n",
       "         'doc': 7,\n",
       "         'discuss': 7,\n",
       "         'pleas': 7,\n",
       "         'advic': 7,\n",
       "         'sister': 7,\n",
       "         'less': 6,\n",
       "         'spent': 6,\n",
       "         'rs': 6,\n",
       "         'min': 6,\n",
       "         'last': 6,\n",
       "         'mother': 6,\n",
       "         'report': 6,\n",
       "         'look': 6,\n",
       "         'high': 6,\n",
       "         'highli': 6,\n",
       "         '!': 6,\n",
       "         'left': 6,\n",
       "         'fever': 6,\n",
       "         'come': 6,\n",
       "         'issuetreat': 6,\n",
       "         'understand': 6,\n",
       "         'charg': 5,\n",
       "         'everyth': 5,\n",
       "         'advis': 5,\n",
       "         'feel': 5,\n",
       "         'better': 5,\n",
       "         'found': 5,\n",
       "         'month': 5,\n",
       "         'respons': 5,\n",
       "         'tablet': 5,\n",
       "         'year': 5,\n",
       "         'famili': 5,\n",
       "         'detail': 5,\n",
       "         'profession': 5,\n",
       "         'bad': 5,\n",
       "         'satisfactionvalu': 5,\n",
       "         'friendlinessexplan': 5,\n",
       "         'total': 4,\n",
       "         'great': 4,\n",
       "         'came': 4,\n",
       "         'second': 4,\n",
       "         'hear': 4,\n",
       "         'proper': 4,\n",
       "         'away': 4,\n",
       "         'interest': 4,\n",
       "         'sinc': 4,\n",
       "         'ill': 4,\n",
       "         'concern': 4,\n",
       "         'god': 4,\n",
       "         'expect': 4,\n",
       "         'told': 4,\n",
       "         'patienc': 4,\n",
       "         'clean': 4,\n",
       "         'noth': 4,\n",
       "         'without': 4,\n",
       "         'help': 4,\n",
       "         'thank': 4,\n",
       "         'anyth': 4,\n",
       "         'confirm': 4,\n",
       "         'friend': 4,\n",
       "         'diagnosi': 4,\n",
       "         'quit': 4,\n",
       "         'pay': 4,\n",
       "         'member': 4,\n",
       "         'thing': 4,\n",
       "         'approach': 4,\n",
       "         'cure': 4,\n",
       "         'posit': 4,\n",
       "         'immedi': 4,\n",
       "         'plz': 4,\n",
       "         'import': 4,\n",
       "         'person': 4,\n",
       "         'made': 4,\n",
       "         'infect': 4,\n",
       "         'meet': 4,\n",
       "         'readi': 4,\n",
       "         'initi': 3,\n",
       "         '-': 3,\n",
       "         'rupe': 3,\n",
       "         'provid': 3,\n",
       "         'nose': 3,\n",
       "         'throat': 3,\n",
       "         'make': 3,\n",
       "         'request': 3,\n",
       "         'spend': 3,\n",
       "         'situat': 3,\n",
       "         'complet': 3,\n",
       "         'differ': 3,\n",
       "         'extrem': 3,\n",
       "         'actual': 3,\n",
       "         'cool': 3,\n",
       "         'simpl': 3,\n",
       "         'ok': 3,\n",
       "         'post': 3,\n",
       "         'cours': 3,\n",
       "         'condit': 3,\n",
       "         'start': 3,\n",
       "         'strongli': 3,\n",
       "         'improv': 3,\n",
       "         'emerg': 3,\n",
       "         'self': 3,\n",
       "         'think': 3,\n",
       "         'felt': 3,\n",
       "         'lot': 3,\n",
       "         'overal': 3,\n",
       "         'servic': 3,\n",
       "         'got': 3,\n",
       "         'disappoint': 3,\n",
       "         'front': 3,\n",
       "         'guy': 3,\n",
       "         'harsh': 3,\n",
       "         'wrong': 3,\n",
       "         'place': 3,\n",
       "         'pretti': 3,\n",
       "         'valu': 3,\n",
       "         'follow': 3,\n",
       "         'touch': 3,\n",
       "         'hurri': 3,\n",
       "         '``': 3,\n",
       "         \"''\": 3,\n",
       "         'travel': 3,\n",
       "         'long': 3,\n",
       "         'question': 3,\n",
       "         'sad': 3,\n",
       "         'staff': 3,\n",
       "         'wife': 3,\n",
       "         'skin': 3,\n",
       "         'alway': 3,\n",
       "         'breath': 3,\n",
       "         'blood': 3,\n",
       "         'facil': 3,\n",
       "         'clearli': 3,\n",
       "         'within': 3,\n",
       "         'doctori': 3,\n",
       "         'arriv': 3,\n",
       "         'reach': 3,\n",
       "         'nd': 3,\n",
       "         'inform': 3,\n",
       "         'exactli': 3,\n",
       "         'want': 3,\n",
       "         'sugar': 3,\n",
       "         'book': 3,\n",
       "         'busi': 3,\n",
       "         'best': 3,\n",
       "         'properli': 3,\n",
       "         'type': 3,\n",
       "         'sy': 3,\n",
       "         'step': 3,\n",
       "         'moneywait': 3,\n",
       "         'cold': 3,\n",
       "         'cough': 3,\n",
       "         'investig': 3,\n",
       "         'costli': 3,\n",
       "         'reason': 3,\n",
       "         'given': 3,\n",
       "         'receiv': 3,\n",
       "         'expir': 3,\n",
       "         'refund': 3,\n",
       "         'fals': 2,\n",
       "         'definit': 2,\n",
       "         'night': 2,\n",
       "         'sens': 2,\n",
       "         '?': 2,\n",
       "         'per': 2,\n",
       "         'west': 2,\n",
       "         'acut': 2,\n",
       "         'sever': 2,\n",
       "         'x-ray': 2,\n",
       "         'murali': 2,\n",
       "         'happen': 2,\n",
       "         'histori': 2,\n",
       "         'jubile': 2,\n",
       "         'hill': 2,\n",
       "         'price': 2,\n",
       "         'perfect': 2,\n",
       "         'worst': 2,\n",
       "         'fake': 2,\n",
       "         'frankli': 2,\n",
       "         'wish': 2,\n",
       "         'life': 2,\n",
       "         'resolv': 2,\n",
       "         'suffer': 2,\n",
       "         'offic': 2,\n",
       "         'prepar': 2,\n",
       "         'wrote': 2,\n",
       "         'forc': 2,\n",
       "         'shop': 2,\n",
       "         'diagnos': 2,\n",
       "         'bill': 2,\n",
       "         'teeth': 2,\n",
       "         'open': 2,\n",
       "         'qualif': 2,\n",
       "         'stay': 2,\n",
       "         'calm': 2,\n",
       "         'almost': 2,\n",
       "         'examin': 2,\n",
       "         'dissatisfi': 2,\n",
       "         'behaviour': 2,\n",
       "         'yell': 2,\n",
       "         'everytim': 2,\n",
       "         'right': 2,\n",
       "         'scam': 2,\n",
       "         'term': 2,\n",
       "         'etc': 2,\n",
       "         'research': 2,\n",
       "         'correct': 2,\n",
       "         'period': 2,\n",
       "         'wast': 2,\n",
       "         'taken': 2,\n",
       "         'abl': 2,\n",
       "         'toward': 2,\n",
       "         'still': 2,\n",
       "         'paid': 2,\n",
       "         'hous': 2,\n",
       "         'recov': 2,\n",
       "         'soon': 2,\n",
       "         'critic': 2,\n",
       "         'chest': 2,\n",
       "         'murthi': 2,\n",
       "         'pm': 2,\n",
       "         'observ': 2,\n",
       "         'mithil': 2,\n",
       "         'awesom': 2,\n",
       "         'recomend': 2,\n",
       "         'known': 2,\n",
       "         'himw': 2,\n",
       "         'gud': 2,\n",
       "         'km': 2,\n",
       "         'heavi': 2,\n",
       "         'traffic': 2,\n",
       "         'morethan': 2,\n",
       "         'issuevalu': 2,\n",
       "         'cream': 2,\n",
       "         'manner': 2,\n",
       "         'behavior': 2,\n",
       "         'stage': 2,\n",
       "         'normal': 2,\n",
       "         'rang': 2,\n",
       "         'level': 2,\n",
       "         'point': 2,\n",
       "         'team': 2,\n",
       "         'avoid': 2,\n",
       "         'dengu': 2,\n",
       "         'full': 2,\n",
       "         'link': 2,\n",
       "         'past': 2,\n",
       "         'insulin': 2,\n",
       "         'sir': 2,\n",
       "         'cordial': 2,\n",
       "         'proscrib': 2,\n",
       "         'pain': 2,\n",
       "         'late': 2,\n",
       "         'everi': 2,\n",
       "         'satisfact': 2,\n",
       "         'seem': 2,\n",
       "         'bother': 2,\n",
       "         'show': 2,\n",
       "         'outsid': 2,\n",
       "         'ever': 2,\n",
       "         'chemic': 2,\n",
       "         'liter': 2,\n",
       "         'directli': 2,\n",
       "         'round': 2,\n",
       "         'free': 2,\n",
       "         'acn': 2,\n",
       "         'refer': 2,\n",
       "         'week': 2,\n",
       "         'relat': 2,\n",
       "         'instead': 2,\n",
       "         'may': 2,\n",
       "         'symptom': 2,\n",
       "         'checkup': 2,\n",
       "         'fine': 2,\n",
       "         'attitud': 2,\n",
       "         'wo': 2,\n",
       "         'face': 2,\n",
       "         'minimum': 2,\n",
       "         'doubt': 2,\n",
       "         'could': 2,\n",
       "         'surgeri': 2,\n",
       "         'requir': 2,\n",
       "         'drug': 2,\n",
       "         'fraud': 2,\n",
       "         'later': 2,\n",
       "         'site': 1,\n",
       "         'includ': 1,\n",
       "         'medicinesth': 1,\n",
       "         'estim': 1,\n",
       "         'solut': 1,\n",
       "         'avil': 1,\n",
       "         'perform': 1,\n",
       "         'human': 1,\n",
       "         'ground': 1,\n",
       "         'atleast': 1,\n",
       "         'remonden': 1,\n",
       "         'pictur': 1,\n",
       "         'amount': 1,\n",
       "         'europ': 1,\n",
       "         'kindli': 1,\n",
       "         'inherit': 1,\n",
       "         'portion': 1,\n",
       "         'bronchiti': 1,\n",
       "         'laring': 1,\n",
       "         'awar': 1,\n",
       "         'earlier': 1,\n",
       "         'understood': 1,\n",
       "         'dy': 1,\n",
       "         'compos': 1,\n",
       "         'seriou': 1,\n",
       "         'least': 1,\n",
       "         'propos': 1,\n",
       "         'specialist': 1,\n",
       "         'alreadi': 1,\n",
       "         'dermatologist': 1,\n",
       "         'decent': 1,\n",
       "         'usag': 1,\n",
       "         'effect': 1,\n",
       "         'bell': 1,\n",
       "         'palsi': 1,\n",
       "         'realist': 1,\n",
       "         'qualifi': 1,\n",
       "         'play': 1,\n",
       "         'cheat': 1,\n",
       "         'though': 1,\n",
       "         'moder': 1,\n",
       "         'main': 1,\n",
       "         'agenda': 1,\n",
       "         'acur': 1,\n",
       "         'name': 1,\n",
       "         'joy': 1,\n",
       "         'dial': 1,\n",
       "         'mobnumb': 1,\n",
       "         'talkingh': 1,\n",
       "         'gentlemani': 1,\n",
       "         'luck': 1,\n",
       "         'futur': 1,\n",
       "         'lifei': 1,\n",
       "         'gratefulto': 1,\n",
       "         'nobl': 1,\n",
       "         'charact': 1,\n",
       "         'famou': 1,\n",
       "         'gti': 1,\n",
       "         'loss': 1,\n",
       "         'lotion': 1,\n",
       "         'perman': 1,\n",
       "         'run': 1,\n",
       "         'written': 1,\n",
       "         'k': 1,\n",
       "         'room': 1,\n",
       "         'anyon': 1,\n",
       "         'horribl': 1,\n",
       "         'dentist': 1,\n",
       "         'schedul': 1,\n",
       "         'two': 1,\n",
       "         'fill': 1,\n",
       "         'elabor': 1,\n",
       "         'dirti': 1,\n",
       "         'reek': 1,\n",
       "         'urin': 1,\n",
       "         'scare': 1,\n",
       "         'near': 1,\n",
       "         'chair': 1,\n",
       "         'unhygien': 1,\n",
       "         'husaain': 1,\n",
       "         'polit': 1,\n",
       "         'sinus': 1,\n",
       "         'gastric': 1,\n",
       "         \"'m\": 1,\n",
       "         'accept': 1,\n",
       "         'walk-in': 1,\n",
       "         'irrit': 1,\n",
       "         'cross': 1,\n",
       "         'examiningpati': 1,\n",
       "         'remot': 1,\n",
       "         'behav': 1,\n",
       "         'rude': 1,\n",
       "         'whenev': 1,\n",
       "         'rid': 1,\n",
       "         'biswaroop': 1,\n",
       "         'roy': 1,\n",
       "         'chowdhuri': 1,\n",
       "         'chap': 1,\n",
       "         'vulner': 1,\n",
       "         'prove': 1,\n",
       "         'legitimaci': 1,\n",
       "         'indo-vietnam': 1,\n",
       "         'board': 1,\n",
       "         'creation': 1,\n",
       "         'valid': 1,\n",
       "         'upload': 1,\n",
       "         'self-written': 1,\n",
       "         'paper': 1,\n",
       "         'websit': 1,\n",
       "         'claim': 1,\n",
       "         'bullshit': 1,\n",
       "         'head': 1,\n",
       "         'fact': 1,\n",
       "         'wari': 1,\n",
       "         'quack': 1,\n",
       "         'content': 1,\n",
       "         'frame': 1,\n",
       "         'notic': 1,\n",
       "         'whole': 1,\n",
       "         'multipl': 1,\n",
       "         'distanc': 1,\n",
       "         'assum': 1,\n",
       "         'fulli': 1,\n",
       "         'adviceonli': 1,\n",
       "         'cooper': 1,\n",
       "         'professionalperfect': 1,\n",
       "         'hygien': 1,\n",
       "         'clinicgood': 1,\n",
       "         'equip': 1,\n",
       "         'atmosphereprompt': 1,\n",
       "         'access': 1,\n",
       "         'dedic': 1,\n",
       "         'qualiti': 1,\n",
       "         'defin': 1,\n",
       "         'drsoumya': 1,\n",
       "         'diseas': 1,\n",
       "         'issueth': 1,\n",
       "         'let': 1,\n",
       "         'completli': 1,\n",
       "         'inbetween': 1,\n",
       "         'rp': 1,\n",
       "         'rash': 1,\n",
       "         'treatmentw': 1,\n",
       "         'admit': 1,\n",
       "         'surviv': 1,\n",
       "         'grace': 1,\n",
       "         'pathet': 1,\n",
       "         'interrupt': 1,\n",
       "         'loos': 1,\n",
       "         'bare': 1,\n",
       "         'arrog': 1,\n",
       "         'problemthi': 1,\n",
       "         'antibiot': 1,\n",
       "         'problemsh': 1,\n",
       "         'feveri': 1,\n",
       "         'chang': 1,\n",
       "         'xraysh': 1,\n",
       "         'fluid': 1,\n",
       "         'lung': 1,\n",
       "         'pressur': 1,\n",
       "         'stethoscop': 1,\n",
       "         'hernow': 1,\n",
       "         'action': 1,\n",
       "         'mix': 1,\n",
       "         'experience-': 1,\n",
       "         'heal': 1,\n",
       "         'perfectlyabout': 1,\n",
       "         'suppos': 1,\n",
       "         'rao': 1,\n",
       "         'nagar': 1,\n",
       "         'wasnt': 1,\n",
       "         'dealt': 1,\n",
       "         'unfortun': 1,\n",
       "         'ladi': 1,\n",
       "         'recept': 1,\n",
       "         'believ': 1,\n",
       "         'ten': 1,\n",
       "         'perceiv': 1,\n",
       "         'factor': 1,\n",
       "         'fulfil': 1,\n",
       "         'vibe': 1,\n",
       "         'moneydoctor': 1,\n",
       "         'support': 1,\n",
       "         'focus': 1,\n",
       "         'push': 1,\n",
       "         'nonbrand': 1,\n",
       "         'sun': 1,\n",
       "         'occur': 1,\n",
       "         'three': 1,\n",
       "         'listion': 1,\n",
       "         'today': 1,\n",
       "         'onward': 1,\n",
       "         'phone': 1,\n",
       "         'along': 1,\n",
       "         'pp': 1,\n",
       "         'fast': 1,\n",
       "         'higher': 1,\n",
       "         'regular': 1,\n",
       "         'suger': 1,\n",
       "         'practoi': 1,\n",
       "         'approxim': 1,\n",
       "         'turnthey': 1,\n",
       "         'serv': 1,\n",
       "         'mean': 1,\n",
       "         'appt': 1,\n",
       "         'excel': 1,\n",
       "         'wonder': 1,\n",
       "         'tackl': 1,\n",
       "         'r': 1,\n",
       "         'quer': 1,\n",
       "         'low': 1,\n",
       "         'empathi': 1,\n",
       "         'leav': 1,\n",
       "         'his/her': 1,\n",
       "         'address': 1,\n",
       "         '+': 1,\n",
       "         'regard': 1,\n",
       "         'effici': 1,\n",
       "         'young': 1,\n",
       "         'beyond': 1,\n",
       "         'hail': 1,\n",
       "         'background': 1,\n",
       "         'evalu': 1,\n",
       "         'dont': 1,\n",
       "         'absolut': 1,\n",
       "         'prescript': 1,\n",
       "         'cbp': 1,\n",
       "         'widal': 1,\n",
       "         'typhoid': 1,\n",
       "         'thoroughli': 1,\n",
       "         'reliabl': 1,\n",
       "         'bewar': 1,\n",
       "         'paramed': 1,\n",
       "         'regist': 1,\n",
       "         'india': 1,\n",
       "         'anywher': 1,\n",
       "         'world': 1,\n",
       "         'read': 1,\n",
       "         'stori': 1,\n",
       "         'eye': 1,\n",
       "         'copi': 1,\n",
       "         'speed': 1,\n",
       "         'slow': 1,\n",
       "         'especi': 1,\n",
       "         'speedi': 1,\n",
       "         'amic': 1,\n",
       "         'henc': 1,\n",
       "         'assur': 1,\n",
       "         'control': 1,\n",
       "         'oral': 1,\n",
       "         'comfort': 1,\n",
       "         'athar': 1,\n",
       "         'hussainhi': 1,\n",
       "         'caus': 1,\n",
       "         'radiologist': 1,\n",
       "         'twice': 1,\n",
       "         'satisfactionexplan': 1,\n",
       "         'knew': 1,\n",
       "         'base': 1,\n",
       "         'prevent': 1,\n",
       "         ';': 1,\n",
       "         'hardli': 1,\n",
       "         'quick': 1,\n",
       "         'complic': 1,\n",
       "         'suitabl': 1,\n",
       "         'return': 1,\n",
       "         'hewa': 1,\n",
       "         'fix': 1,\n",
       "         'sympathi': 1,\n",
       "         'kept': 1,\n",
       "         'nurs': 1,\n",
       "         'earli': 1,\n",
       "         'possibl': 1,\n",
       "         'lessfinanci': 1,\n",
       "         'aspect': 1,\n",
       "         'unnecessari': 1,\n",
       "         'treatmentdoctor': 1,\n",
       "         'other': 1,\n",
       "         'common': 1,\n",
       "         'thought': 1,\n",
       "         'littl': 1,\n",
       "         'shiver': 1,\n",
       "         'proceed': 1,\n",
       "         'outcom': 1,\n",
       "         'ear': 1,\n",
       "         'indepth': 1,\n",
       "         'neat': 1,\n",
       "         'everyon': 1,\n",
       "         'samealway': 1,\n",
       "         'compassion': 1,\n",
       "         'careand': 1,\n",
       "         'unwant': 1,\n",
       "         'demand': 1,\n",
       "         'costlydid': 1,\n",
       "         'affair': 1,\n",
       "         '@': 1,\n",
       "         'numb': 1,\n",
       "         'hand': 1,\n",
       "         'kondapur': 1,\n",
       "         'insist': 1,\n",
       "         'doctormi': 1,\n",
       "         'peelsh': 1,\n",
       "         'hi': 1,\n",
       "         'resulta': 1,\n",
       "         'moneyimmedi': 1,\n",
       "         'tell': 1,\n",
       "         'atm': 1,\n",
       "         'bring': 1,\n",
       "         'wise': 1,\n",
       "         'visibleso': 1,\n",
       "         'cab': 1,\n",
       "         'upagain': 1,\n",
       "         'hersh': 1,\n",
       "         'moneyth': 1,\n",
       "         'worknow': 1,\n",
       "         'yesterday': 1,\n",
       "         'buy': 1,\n",
       "         'medicinemi': 1,\n",
       "         'disgust': 1,\n",
       "         'amoun': 1,\n",
       "         'shell': 1,\n",
       "         'unbeliev': 1,\n",
       "         'pigment': 1,\n",
       "         'kind': 1,\n",
       "         'variou': 1,\n",
       "         'aria': 1,\n",
       "         'tri': 1,\n",
       "         'soumya': 1,\n",
       "         'shown': 1,\n",
       "         'practo': 1,\n",
       "         'app': 1,\n",
       "         'neither': 1,\n",
       "         'mayfair': 1,\n",
       "         'garden': 1,\n",
       "         'resultmed': 1,\n",
       "         'exact': 1,\n",
       "         'sit': 1,\n",
       "         'array': 1,\n",
       "         'despit': 1,\n",
       "         'major': 1,\n",
       "         'info': 1,\n",
       "         'set': 1,\n",
       "         'donefrom': 1,\n",
       "         'ofcours': 1,\n",
       "         'across': 1,\n",
       "         'manag': 1,\n",
       "         'diet': 1,\n",
       "         'lifestyl': 1,\n",
       "         'unless': 1,\n",
       "         'specif': 1,\n",
       "         'pathatich': 1,\n",
       "         'list': 1,\n",
       "         'promptth': 1,\n",
       "         'handl': 1,\n",
       "         'humbl': 1,\n",
       "         'heard': 1,\n",
       "         'broadli': 1,\n",
       "         'messag': 1,\n",
       "         'promptli': 1,\n",
       "         'easi': 1,\n",
       "         'viral': 1,\n",
       "         'suspect': 1,\n",
       "         'malaria/typhoid': 1,\n",
       "         'dosag': 1,\n",
       "         'peoplewhi': 1,\n",
       "         'man': 1,\n",
       "         'di': 1,\n",
       "         'realitya': 1,\n",
       "         'end': 1,\n",
       "         'relax': 1,\n",
       "         'ampl': 1,\n",
       "         'middl': 1,\n",
       "         'class': 1,\n",
       "         'budget': 1,\n",
       "         'requst': 1,\n",
       "         'thier': 1,\n",
       "         'econom': 1,\n",
       "         'necessari': 1,\n",
       "         'habbilt': 1,\n",
       "         'acknowledg': 1,\n",
       "         'asksforget': 1,\n",
       "         'answer': 1,\n",
       "         'around': 1,\n",
       "         'courteou': 1,\n",
       "         'respect': 1,\n",
       "         'unprofession': 1,\n",
       "         'deceas': 1,\n",
       "         'otherwis': 1,\n",
       "         'extra': 1,\n",
       "         'greatli': 1,\n",
       "         'determin': 1,\n",
       "         'highso': 1,\n",
       "         'fair': 1,\n",
       "         'home': 1,\n",
       "         'exchang': 1,\n",
       "         'shout': 1,\n",
       "         'sowmya': 1,\n",
       "         'welcom': 1,\n",
       "         'pleasant': 1,\n",
       "         'smilesh': 1,\n",
       "         'clearley': 1,\n",
       "         'precaut': 1,\n",
       "         'takew': 1,\n",
       "         'husband': 1,\n",
       "         'medicationsgot': 1,\n",
       "         'weekw': 1,\n",
       "         'treartmentsh': 1,\n",
       "         'rush': 1,\n",
       "         'doctorssh': 1,\n",
       "         'medicationgood': 1,\n",
       "         'doctoronli': 1,\n",
       "         'straight': 1,\n",
       "         'prefer': 1,\n",
       "         'im': 1,\n",
       "         '%': 1,\n",
       "         'percent': 1,\n",
       "         'mark': 1,\n",
       "         'commun': 1,\n",
       "         'keep': 1,\n",
       "         'chat': 1,\n",
       "         'mobil': 1,\n",
       "         'among': 1,\n",
       "         'top': 1,\n",
       "         'endocrinologist': 1,\n",
       "         'rupeesi': 1,\n",
       "         'orphanag': 1,\n",
       "         'vari': 1,\n",
       "         'find': 1,\n",
       "         'spot': 1,\n",
       "         'trust': 1,\n",
       "         'responc': 1,\n",
       "         'satisfactori': 1,\n",
       "         'glad': 1,\n",
       "         'met': 1,\n",
       "         'bless': 1,\n",
       "         'firstli': 1,\n",
       "         'moredoctor': 1,\n",
       "         'tonsil': 1,\n",
       "         'temporari': 1,\n",
       "         'relief': 1,\n",
       "         'procedur': 1,\n",
       "         'fraction': 1,\n",
       "         'surgey': 1,\n",
       "         'whether': 1,\n",
       "         'peel': 1,\n",
       "         'identif': 1,\n",
       "         'goodexplan': 1,\n",
       "         'mint': 1,\n",
       "         'h': 1,\n",
       "         'pylori': 1,\n",
       "         'patientin': 1,\n",
       "         'blousesaft': 1,\n",
       "         'wash': 1,\n",
       "         'handsi': 1,\n",
       "         'anywherefor': 1,\n",
       "         'rupeestot': 1,\n",
       "         'kushi': 1,\n",
       "         'registr': 1,\n",
       "         'intent': 1,\n",
       "         'collect': 1,\n",
       "         'laksh': 1,\n",
       "         'u': 1,\n",
       "         'progress': 1,\n",
       "         'zero': 1,\n",
       "         'experiencedlisten': 1,\n",
       "         'problemsexplain': 1,\n",
       "         'treatmentoveral': 1,\n",
       "         'medicinereli': 1,\n",
       "         'part': 1,\n",
       "         'colonoscopi': 1,\n",
       "         'availbl': 1,\n",
       "         'nijampet': 1,\n",
       "         'burri': 1,\n",
       "         'diabet': 1,\n",
       "         'unkind': 1,\n",
       "         'sometim': 1,\n",
       "         'ban': 1,\n",
       "         'remark': 1,\n",
       "         'plan': 1,\n",
       "         'outstand': 1,\n",
       "         'safe': 1,\n",
       "         'matter': 1,\n",
       "         'ur': 1,\n",
       "         'strong': 1,\n",
       "         'bookedso': 1,\n",
       "         'warn': 1,\n",
       "         'degre': 1,\n",
       "         'idea': 1,\n",
       "         'medicalh': 1,\n",
       "         'kill': 1,\n",
       "         'chronic': 1,\n",
       "         'big': 1,\n",
       "         'protect': 1,\n",
       "         'instruct': 1,\n",
       "         'write': 1,\n",
       "         'assist': 1,\n",
       "         'chanc': 1,\n",
       "         'eat': 1,\n",
       "         'physic': 1,\n",
       "         'activ': 1,\n",
       "         'irregular': 1,\n",
       "         'review': 1,\n",
       "         'bodi': 1,\n",
       "         'urgent': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6764926-d7f8-402a-9973-d61a03b06461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd66b89-66ee-4da5-aa3d-78634f2719ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62630297-9e17-4ccf-a21d-f65b033df289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[key for key in vocab if vocab[key]>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad52632c-85b5-468d-b2d7-42977e49a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'physiotherapist': 110,\n",
       "         'care': 47,\n",
       "         'patient': 46,\n",
       "         'time': 44,\n",
       "         'medic': 43,\n",
       "         'recommend': 33,\n",
       "         'test': 33,\n",
       "         'good': 31,\n",
       "         'explain': 30,\n",
       "         \"n't\": 29,\n",
       "         'problem': 28,\n",
       "         'visit': 26,\n",
       "         'appoint': 23,\n",
       "         'hospit': 22,\n",
       "         'experi': 21,\n",
       "         'dr': 20,\n",
       "         'friendli': 19,\n",
       "         'issu': 19,\n",
       "         'wait': 19,\n",
       "         'consult': 18,\n",
       "         'day': 17,\n",
       "         'â€™': 17,\n",
       "         'said': 17,\n",
       "         'ask': 17,\n",
       "         'satisfi': 16,\n",
       "         'health': 16,\n",
       "         'get': 16,\n",
       "         'suggest': 16,\n",
       "         'like': 15,\n",
       "         'gave': 15,\n",
       "         'happi': 15,\n",
       "         \"'s\": 14,\n",
       "         'need': 14,\n",
       "         'give': 14,\n",
       "         'even': 14,\n",
       "         'check': 14,\n",
       "         'take': 13,\n",
       "         'listen': 13,\n",
       "         'go': 12,\n",
       "         'explan': 12,\n",
       "         'way': 12,\n",
       "         'onlin': 12,\n",
       "         'went': 12,\n",
       "         'peopl': 11,\n",
       "         'avail': 11,\n",
       "         'first': 11,\n",
       "         'nice': 10,\n",
       "         'anoth': 10,\n",
       "         'treat': 10,\n",
       "         'money': 10,\n",
       "         'mani': 10,\n",
       "         'platform': 10,\n",
       "         'took': 10,\n",
       "         'realli': 10,\n",
       "         '&': 9,\n",
       "         'one': 9,\n",
       "         'well': 9,\n",
       "         'apollo': 9,\n",
       "         'result': 9,\n",
       "         'also': 9,\n",
       "         'doctor': 9,\n",
       "         'medicin': 9,\n",
       "         'would': 9,\n",
       "         'hour': 9,\n",
       "         'prescrib': 9,\n",
       "         'cost': 8,\n",
       "         'see': 8,\n",
       "         'talk': 8,\n",
       "         'us': 8,\n",
       "         'say': 8,\n",
       "         'minut': 8,\n",
       "         'done': 8,\n",
       "         'call': 8,\n",
       "         'much': 8,\n",
       "         'endoscopi': 7,\n",
       "         'telemedicin': 7,\n",
       "         'know': 7,\n",
       "         'next': 7,\n",
       "         'hope': 7,\n",
       "         'experienc': 7,\n",
       "         'work': 7,\n",
       "         'use': 7,\n",
       "         'never': 7,\n",
       "         'doc': 7,\n",
       "         'discuss': 7,\n",
       "         'pleas': 7,\n",
       "         'advic': 7,\n",
       "         'sister': 7,\n",
       "         'less': 6,\n",
       "         'spent': 6,\n",
       "         'rs': 6,\n",
       "         'min': 6,\n",
       "         'last': 6,\n",
       "         'mother': 6,\n",
       "         'report': 6,\n",
       "         'look': 6,\n",
       "         'high': 6,\n",
       "         'highli': 6,\n",
       "         '!': 6,\n",
       "         'left': 6,\n",
       "         'fever': 6,\n",
       "         'come': 6,\n",
       "         'issuetreat': 6,\n",
       "         'understand': 6,\n",
       "         'charg': 5,\n",
       "         'everyth': 5,\n",
       "         'advis': 5,\n",
       "         'feel': 5,\n",
       "         'better': 5,\n",
       "         'found': 5,\n",
       "         'month': 5,\n",
       "         'respons': 5,\n",
       "         'tablet': 5,\n",
       "         'year': 5,\n",
       "         'famili': 5,\n",
       "         'detail': 5,\n",
       "         'profession': 5,\n",
       "         'bad': 5,\n",
       "         'satisfactionvalu': 5,\n",
       "         'friendlinessexplan': 5,\n",
       "         'total': 4,\n",
       "         'great': 4,\n",
       "         'came': 4,\n",
       "         'second': 4,\n",
       "         'hear': 4,\n",
       "         'proper': 4,\n",
       "         'away': 4,\n",
       "         'interest': 4,\n",
       "         'sinc': 4,\n",
       "         'ill': 4,\n",
       "         'concern': 4,\n",
       "         'god': 4,\n",
       "         'expect': 4,\n",
       "         'told': 4,\n",
       "         'patienc': 4,\n",
       "         'clean': 4,\n",
       "         'noth': 4,\n",
       "         'without': 4,\n",
       "         'help': 4,\n",
       "         'thank': 4,\n",
       "         'anyth': 4,\n",
       "         'confirm': 4,\n",
       "         'friend': 4,\n",
       "         'diagnosi': 4,\n",
       "         'quit': 4,\n",
       "         'pay': 4,\n",
       "         'member': 4,\n",
       "         'thing': 4,\n",
       "         'approach': 4,\n",
       "         'cure': 4,\n",
       "         'posit': 4,\n",
       "         'immedi': 4,\n",
       "         'plz': 4,\n",
       "         'import': 4,\n",
       "         'person': 4,\n",
       "         'made': 4,\n",
       "         'infect': 4,\n",
       "         'meet': 4,\n",
       "         'readi': 4,\n",
       "         'initi': 3,\n",
       "         '-': 3,\n",
       "         'rupe': 3,\n",
       "         'provid': 3,\n",
       "         'nose': 3,\n",
       "         'throat': 3,\n",
       "         'make': 3,\n",
       "         'request': 3,\n",
       "         'spend': 3,\n",
       "         'situat': 3,\n",
       "         'complet': 3,\n",
       "         'differ': 3,\n",
       "         'extrem': 3,\n",
       "         'actual': 3,\n",
       "         'cool': 3,\n",
       "         'simpl': 3,\n",
       "         'ok': 3,\n",
       "         'post': 3,\n",
       "         'cours': 3,\n",
       "         'condit': 3,\n",
       "         'start': 3,\n",
       "         'strongli': 3,\n",
       "         'improv': 3,\n",
       "         'emerg': 3,\n",
       "         'self': 3,\n",
       "         'think': 3,\n",
       "         'felt': 3,\n",
       "         'lot': 3,\n",
       "         'overal': 3,\n",
       "         'servic': 3,\n",
       "         'got': 3,\n",
       "         'disappoint': 3,\n",
       "         'front': 3,\n",
       "         'guy': 3,\n",
       "         'harsh': 3,\n",
       "         'wrong': 3,\n",
       "         'place': 3,\n",
       "         'pretti': 3,\n",
       "         'valu': 3,\n",
       "         'follow': 3,\n",
       "         'touch': 3,\n",
       "         'hurri': 3,\n",
       "         '``': 3,\n",
       "         \"''\": 3,\n",
       "         'travel': 3,\n",
       "         'long': 3,\n",
       "         'question': 3,\n",
       "         'sad': 3,\n",
       "         'staff': 3,\n",
       "         'wife': 3,\n",
       "         'skin': 3,\n",
       "         'alway': 3,\n",
       "         'breath': 3,\n",
       "         'blood': 3,\n",
       "         'facil': 3,\n",
       "         'clearli': 3,\n",
       "         'within': 3,\n",
       "         'doctori': 3,\n",
       "         'arriv': 3,\n",
       "         'reach': 3,\n",
       "         'nd': 3,\n",
       "         'inform': 3,\n",
       "         'exactli': 3,\n",
       "         'want': 3,\n",
       "         'sugar': 3,\n",
       "         'book': 3,\n",
       "         'busi': 3,\n",
       "         'best': 3,\n",
       "         'properli': 3,\n",
       "         'type': 3,\n",
       "         'sy': 3,\n",
       "         'step': 3,\n",
       "         'moneywait': 3,\n",
       "         'cold': 3,\n",
       "         'cough': 3,\n",
       "         'investig': 3,\n",
       "         'costli': 3,\n",
       "         'reason': 3,\n",
       "         'given': 3,\n",
       "         'receiv': 3,\n",
       "         'expir': 3,\n",
       "         'refund': 3,\n",
       "         'fals': 2,\n",
       "         'definit': 2,\n",
       "         'night': 2,\n",
       "         'sens': 2,\n",
       "         '?': 2,\n",
       "         'per': 2,\n",
       "         'west': 2,\n",
       "         'acut': 2,\n",
       "         'sever': 2,\n",
       "         'x-ray': 2,\n",
       "         'murali': 2,\n",
       "         'happen': 2,\n",
       "         'histori': 2,\n",
       "         'jubile': 2,\n",
       "         'hill': 2,\n",
       "         'price': 2,\n",
       "         'perfect': 2,\n",
       "         'worst': 2,\n",
       "         'fake': 2,\n",
       "         'frankli': 2,\n",
       "         'wish': 2,\n",
       "         'life': 2,\n",
       "         'resolv': 2,\n",
       "         'suffer': 2,\n",
       "         'offic': 2,\n",
       "         'prepar': 2,\n",
       "         'wrote': 2,\n",
       "         'forc': 2,\n",
       "         'shop': 2,\n",
       "         'diagnos': 2,\n",
       "         'bill': 2,\n",
       "         'teeth': 2,\n",
       "         'open': 2,\n",
       "         'qualif': 2,\n",
       "         'stay': 2,\n",
       "         'calm': 2,\n",
       "         'almost': 2,\n",
       "         'examin': 2,\n",
       "         'dissatisfi': 2,\n",
       "         'behaviour': 2,\n",
       "         'yell': 2,\n",
       "         'everytim': 2,\n",
       "         'right': 2,\n",
       "         'scam': 2,\n",
       "         'term': 2,\n",
       "         'etc': 2,\n",
       "         'research': 2,\n",
       "         'correct': 2,\n",
       "         'period': 2,\n",
       "         'wast': 2,\n",
       "         'taken': 2,\n",
       "         'abl': 2,\n",
       "         'toward': 2,\n",
       "         'still': 2,\n",
       "         'paid': 2,\n",
       "         'hous': 2,\n",
       "         'recov': 2,\n",
       "         'soon': 2,\n",
       "         'critic': 2,\n",
       "         'chest': 2,\n",
       "         'murthi': 2,\n",
       "         'pm': 2,\n",
       "         'observ': 2,\n",
       "         'mithil': 2,\n",
       "         'awesom': 2,\n",
       "         'recomend': 2,\n",
       "         'known': 2,\n",
       "         'himw': 2,\n",
       "         'gud': 2,\n",
       "         'km': 2,\n",
       "         'heavi': 2,\n",
       "         'traffic': 2,\n",
       "         'morethan': 2,\n",
       "         'issuevalu': 2,\n",
       "         'cream': 2,\n",
       "         'manner': 2,\n",
       "         'behavior': 2,\n",
       "         'stage': 2,\n",
       "         'normal': 2,\n",
       "         'rang': 2,\n",
       "         'level': 2,\n",
       "         'point': 2,\n",
       "         'team': 2,\n",
       "         'avoid': 2,\n",
       "         'dengu': 2,\n",
       "         'full': 2,\n",
       "         'link': 2,\n",
       "         'past': 2,\n",
       "         'insulin': 2,\n",
       "         'sir': 2,\n",
       "         'cordial': 2,\n",
       "         'proscrib': 2,\n",
       "         'pain': 2,\n",
       "         'late': 2,\n",
       "         'everi': 2,\n",
       "         'satisfact': 2,\n",
       "         'seem': 2,\n",
       "         'bother': 2,\n",
       "         'show': 2,\n",
       "         'outsid': 2,\n",
       "         'ever': 2,\n",
       "         'chemic': 2,\n",
       "         'liter': 2,\n",
       "         'directli': 2,\n",
       "         'round': 2,\n",
       "         'free': 2,\n",
       "         'acn': 2,\n",
       "         'refer': 2,\n",
       "         'week': 2,\n",
       "         'relat': 2,\n",
       "         'instead': 2,\n",
       "         'may': 2,\n",
       "         'symptom': 2,\n",
       "         'checkup': 2,\n",
       "         'fine': 2,\n",
       "         'attitud': 2,\n",
       "         'wo': 2,\n",
       "         'face': 2,\n",
       "         'minimum': 2,\n",
       "         'doubt': 2,\n",
       "         'could': 2,\n",
       "         'surgeri': 2,\n",
       "         'requir': 2,\n",
       "         'drug': 2,\n",
       "         'fraud': 2,\n",
       "         'later': 2,\n",
       "         'site': 1,\n",
       "         'includ': 1,\n",
       "         'medicinesth': 1,\n",
       "         'estim': 1,\n",
       "         'solut': 1,\n",
       "         'avil': 1,\n",
       "         'perform': 1,\n",
       "         'human': 1,\n",
       "         'ground': 1,\n",
       "         'atleast': 1,\n",
       "         'remonden': 1,\n",
       "         'pictur': 1,\n",
       "         'amount': 1,\n",
       "         'europ': 1,\n",
       "         'kindli': 1,\n",
       "         'inherit': 1,\n",
       "         'portion': 1,\n",
       "         'bronchiti': 1,\n",
       "         'laring': 1,\n",
       "         'awar': 1,\n",
       "         'earlier': 1,\n",
       "         'understood': 1,\n",
       "         'dy': 1,\n",
       "         'compos': 1,\n",
       "         'seriou': 1,\n",
       "         'least': 1,\n",
       "         'propos': 1,\n",
       "         'specialist': 1,\n",
       "         'alreadi': 1,\n",
       "         'dermatologist': 1,\n",
       "         'decent': 1,\n",
       "         'usag': 1,\n",
       "         'effect': 1,\n",
       "         'bell': 1,\n",
       "         'palsi': 1,\n",
       "         'realist': 1,\n",
       "         'qualifi': 1,\n",
       "         'play': 1,\n",
       "         'cheat': 1,\n",
       "         'though': 1,\n",
       "         'moder': 1,\n",
       "         'main': 1,\n",
       "         'agenda': 1,\n",
       "         'acur': 1,\n",
       "         'name': 1,\n",
       "         'joy': 1,\n",
       "         'dial': 1,\n",
       "         'mobnumb': 1,\n",
       "         'talkingh': 1,\n",
       "         'gentlemani': 1,\n",
       "         'luck': 1,\n",
       "         'futur': 1,\n",
       "         'lifei': 1,\n",
       "         'gratefulto': 1,\n",
       "         'nobl': 1,\n",
       "         'charact': 1,\n",
       "         'famou': 1,\n",
       "         'gti': 1,\n",
       "         'loss': 1,\n",
       "         'lotion': 1,\n",
       "         'perman': 1,\n",
       "         'run': 1,\n",
       "         'written': 1,\n",
       "         'k': 1,\n",
       "         'room': 1,\n",
       "         'anyon': 1,\n",
       "         'horribl': 1,\n",
       "         'dentist': 1,\n",
       "         'schedul': 1,\n",
       "         'two': 1,\n",
       "         'fill': 1,\n",
       "         'elabor': 1,\n",
       "         'dirti': 1,\n",
       "         'reek': 1,\n",
       "         'urin': 1,\n",
       "         'scare': 1,\n",
       "         'near': 1,\n",
       "         'chair': 1,\n",
       "         'unhygien': 1,\n",
       "         'husaain': 1,\n",
       "         'polit': 1,\n",
       "         'sinus': 1,\n",
       "         'gastric': 1,\n",
       "         \"'m\": 1,\n",
       "         'accept': 1,\n",
       "         'walk-in': 1,\n",
       "         'irrit': 1,\n",
       "         'cross': 1,\n",
       "         'examiningpati': 1,\n",
       "         'remot': 1,\n",
       "         'behav': 1,\n",
       "         'rude': 1,\n",
       "         'whenev': 1,\n",
       "         'rid': 1,\n",
       "         'biswaroop': 1,\n",
       "         'roy': 1,\n",
       "         'chowdhuri': 1,\n",
       "         'chap': 1,\n",
       "         'vulner': 1,\n",
       "         'prove': 1,\n",
       "         'legitimaci': 1,\n",
       "         'indo-vietnam': 1,\n",
       "         'board': 1,\n",
       "         'creation': 1,\n",
       "         'valid': 1,\n",
       "         'upload': 1,\n",
       "         'self-written': 1,\n",
       "         'paper': 1,\n",
       "         'websit': 1,\n",
       "         'claim': 1,\n",
       "         'bullshit': 1,\n",
       "         'head': 1,\n",
       "         'fact': 1,\n",
       "         'wari': 1,\n",
       "         'quack': 1,\n",
       "         'content': 1,\n",
       "         'frame': 1,\n",
       "         'notic': 1,\n",
       "         'whole': 1,\n",
       "         'multipl': 1,\n",
       "         'distanc': 1,\n",
       "         'assum': 1,\n",
       "         'fulli': 1,\n",
       "         'adviceonli': 1,\n",
       "         'cooper': 1,\n",
       "         'professionalperfect': 1,\n",
       "         'hygien': 1,\n",
       "         'clinicgood': 1,\n",
       "         'equip': 1,\n",
       "         'atmosphereprompt': 1,\n",
       "         'access': 1,\n",
       "         'dedic': 1,\n",
       "         'qualiti': 1,\n",
       "         'defin': 1,\n",
       "         'drsoumya': 1,\n",
       "         'diseas': 1,\n",
       "         'issueth': 1,\n",
       "         'let': 1,\n",
       "         'completli': 1,\n",
       "         'inbetween': 1,\n",
       "         'rp': 1,\n",
       "         'rash': 1,\n",
       "         'treatmentw': 1,\n",
       "         'admit': 1,\n",
       "         'surviv': 1,\n",
       "         'grace': 1,\n",
       "         'pathet': 1,\n",
       "         'interrupt': 1,\n",
       "         'loos': 1,\n",
       "         'bare': 1,\n",
       "         'arrog': 1,\n",
       "         'problemthi': 1,\n",
       "         'antibiot': 1,\n",
       "         'problemsh': 1,\n",
       "         'feveri': 1,\n",
       "         'chang': 1,\n",
       "         'xraysh': 1,\n",
       "         'fluid': 1,\n",
       "         'lung': 1,\n",
       "         'pressur': 1,\n",
       "         'stethoscop': 1,\n",
       "         'hernow': 1,\n",
       "         'action': 1,\n",
       "         'mix': 1,\n",
       "         'experience-': 1,\n",
       "         'heal': 1,\n",
       "         'perfectlyabout': 1,\n",
       "         'suppos': 1,\n",
       "         'rao': 1,\n",
       "         'nagar': 1,\n",
       "         'wasnt': 1,\n",
       "         'dealt': 1,\n",
       "         'unfortun': 1,\n",
       "         'ladi': 1,\n",
       "         'recept': 1,\n",
       "         'believ': 1,\n",
       "         'ten': 1,\n",
       "         'perceiv': 1,\n",
       "         'factor': 1,\n",
       "         'fulfil': 1,\n",
       "         'vibe': 1,\n",
       "         'moneydoctor': 1,\n",
       "         'support': 1,\n",
       "         'focus': 1,\n",
       "         'push': 1,\n",
       "         'nonbrand': 1,\n",
       "         'sun': 1,\n",
       "         'occur': 1,\n",
       "         'three': 1,\n",
       "         'listion': 1,\n",
       "         'today': 1,\n",
       "         'onward': 1,\n",
       "         'phone': 1,\n",
       "         'along': 1,\n",
       "         'pp': 1,\n",
       "         'fast': 1,\n",
       "         'higher': 1,\n",
       "         'regular': 1,\n",
       "         'suger': 1,\n",
       "         'practoi': 1,\n",
       "         'approxim': 1,\n",
       "         'turnthey': 1,\n",
       "         'serv': 1,\n",
       "         'mean': 1,\n",
       "         'appt': 1,\n",
       "         'excel': 1,\n",
       "         'wonder': 1,\n",
       "         'tackl': 1,\n",
       "         'r': 1,\n",
       "         'quer': 1,\n",
       "         'low': 1,\n",
       "         'empathi': 1,\n",
       "         'leav': 1,\n",
       "         'his/her': 1,\n",
       "         'address': 1,\n",
       "         '+': 1,\n",
       "         'regard': 1,\n",
       "         'effici': 1,\n",
       "         'young': 1,\n",
       "         'beyond': 1,\n",
       "         'hail': 1,\n",
       "         'background': 1,\n",
       "         'evalu': 1,\n",
       "         'dont': 1,\n",
       "         'absolut': 1,\n",
       "         'prescript': 1,\n",
       "         'cbp': 1,\n",
       "         'widal': 1,\n",
       "         'typhoid': 1,\n",
       "         'thoroughli': 1,\n",
       "         'reliabl': 1,\n",
       "         'bewar': 1,\n",
       "         'paramed': 1,\n",
       "         'regist': 1,\n",
       "         'india': 1,\n",
       "         'anywher': 1,\n",
       "         'world': 1,\n",
       "         'read': 1,\n",
       "         'stori': 1,\n",
       "         'eye': 1,\n",
       "         'copi': 1,\n",
       "         'speed': 1,\n",
       "         'slow': 1,\n",
       "         'especi': 1,\n",
       "         'speedi': 1,\n",
       "         'amic': 1,\n",
       "         'henc': 1,\n",
       "         'assur': 1,\n",
       "         'control': 1,\n",
       "         'oral': 1,\n",
       "         'comfort': 1,\n",
       "         'athar': 1,\n",
       "         'hussainhi': 1,\n",
       "         'caus': 1,\n",
       "         'radiologist': 1,\n",
       "         'twice': 1,\n",
       "         'satisfactionexplan': 1,\n",
       "         'knew': 1,\n",
       "         'base': 1,\n",
       "         'prevent': 1,\n",
       "         ';': 1,\n",
       "         'hardli': 1,\n",
       "         'quick': 1,\n",
       "         'complic': 1,\n",
       "         'suitabl': 1,\n",
       "         'return': 1,\n",
       "         'hewa': 1,\n",
       "         'fix': 1,\n",
       "         'sympathi': 1,\n",
       "         'kept': 1,\n",
       "         'nurs': 1,\n",
       "         'earli': 1,\n",
       "         'possibl': 1,\n",
       "         'lessfinanci': 1,\n",
       "         'aspect': 1,\n",
       "         'unnecessari': 1,\n",
       "         'treatmentdoctor': 1,\n",
       "         'other': 1,\n",
       "         'common': 1,\n",
       "         'thought': 1,\n",
       "         'littl': 1,\n",
       "         'shiver': 1,\n",
       "         'proceed': 1,\n",
       "         'outcom': 1,\n",
       "         'ear': 1,\n",
       "         'indepth': 1,\n",
       "         'neat': 1,\n",
       "         'everyon': 1,\n",
       "         'samealway': 1,\n",
       "         'compassion': 1,\n",
       "         'careand': 1,\n",
       "         'unwant': 1,\n",
       "         'demand': 1,\n",
       "         'costlydid': 1,\n",
       "         'affair': 1,\n",
       "         '@': 1,\n",
       "         'numb': 1,\n",
       "         'hand': 1,\n",
       "         'kondapur': 1,\n",
       "         'insist': 1,\n",
       "         'doctormi': 1,\n",
       "         'peelsh': 1,\n",
       "         'hi': 1,\n",
       "         'resulta': 1,\n",
       "         'moneyimmedi': 1,\n",
       "         'tell': 1,\n",
       "         'atm': 1,\n",
       "         'bring': 1,\n",
       "         'wise': 1,\n",
       "         'visibleso': 1,\n",
       "         'cab': 1,\n",
       "         'upagain': 1,\n",
       "         'hersh': 1,\n",
       "         'moneyth': 1,\n",
       "         'worknow': 1,\n",
       "         'yesterday': 1,\n",
       "         'buy': 1,\n",
       "         'medicinemi': 1,\n",
       "         'disgust': 1,\n",
       "         'amoun': 1,\n",
       "         'shell': 1,\n",
       "         'unbeliev': 1,\n",
       "         'pigment': 1,\n",
       "         'kind': 1,\n",
       "         'variou': 1,\n",
       "         'aria': 1,\n",
       "         'tri': 1,\n",
       "         'soumya': 1,\n",
       "         'shown': 1,\n",
       "         'practo': 1,\n",
       "         'app': 1,\n",
       "         'neither': 1,\n",
       "         'mayfair': 1,\n",
       "         'garden': 1,\n",
       "         'resultmed': 1,\n",
       "         'exact': 1,\n",
       "         'sit': 1,\n",
       "         'array': 1,\n",
       "         'despit': 1,\n",
       "         'major': 1,\n",
       "         'info': 1,\n",
       "         'set': 1,\n",
       "         'donefrom': 1,\n",
       "         'ofcours': 1,\n",
       "         'across': 1,\n",
       "         'manag': 1,\n",
       "         'diet': 1,\n",
       "         'lifestyl': 1,\n",
       "         'unless': 1,\n",
       "         'specif': 1,\n",
       "         'pathatich': 1,\n",
       "         'list': 1,\n",
       "         'promptth': 1,\n",
       "         'handl': 1,\n",
       "         'humbl': 1,\n",
       "         'heard': 1,\n",
       "         'broadli': 1,\n",
       "         'messag': 1,\n",
       "         'promptli': 1,\n",
       "         'easi': 1,\n",
       "         'viral': 1,\n",
       "         'suspect': 1,\n",
       "         'malaria/typhoid': 1,\n",
       "         'dosag': 1,\n",
       "         'peoplewhi': 1,\n",
       "         'man': 1,\n",
       "         'di': 1,\n",
       "         'realitya': 1,\n",
       "         'end': 1,\n",
       "         'relax': 1,\n",
       "         'ampl': 1,\n",
       "         'middl': 1,\n",
       "         'class': 1,\n",
       "         'budget': 1,\n",
       "         'requst': 1,\n",
       "         'thier': 1,\n",
       "         'econom': 1,\n",
       "         'necessari': 1,\n",
       "         'habbilt': 1,\n",
       "         'acknowledg': 1,\n",
       "         'asksforget': 1,\n",
       "         'answer': 1,\n",
       "         'around': 1,\n",
       "         'courteou': 1,\n",
       "         'respect': 1,\n",
       "         'unprofession': 1,\n",
       "         'deceas': 1,\n",
       "         'otherwis': 1,\n",
       "         'extra': 1,\n",
       "         'greatli': 1,\n",
       "         'determin': 1,\n",
       "         'highso': 1,\n",
       "         'fair': 1,\n",
       "         'home': 1,\n",
       "         'exchang': 1,\n",
       "         'shout': 1,\n",
       "         'sowmya': 1,\n",
       "         'welcom': 1,\n",
       "         'pleasant': 1,\n",
       "         'smilesh': 1,\n",
       "         'clearley': 1,\n",
       "         'precaut': 1,\n",
       "         'takew': 1,\n",
       "         'husband': 1,\n",
       "         'medicationsgot': 1,\n",
       "         'weekw': 1,\n",
       "         'treartmentsh': 1,\n",
       "         'rush': 1,\n",
       "         'doctorssh': 1,\n",
       "         'medicationgood': 1,\n",
       "         'doctoronli': 1,\n",
       "         'straight': 1,\n",
       "         'prefer': 1,\n",
       "         'im': 1,\n",
       "         '%': 1,\n",
       "         'percent': 1,\n",
       "         'mark': 1,\n",
       "         'commun': 1,\n",
       "         'keep': 1,\n",
       "         'chat': 1,\n",
       "         'mobil': 1,\n",
       "         'among': 1,\n",
       "         'top': 1,\n",
       "         'endocrinologist': 1,\n",
       "         'rupeesi': 1,\n",
       "         'orphanag': 1,\n",
       "         'vari': 1,\n",
       "         'find': 1,\n",
       "         'spot': 1,\n",
       "         'trust': 1,\n",
       "         'responc': 1,\n",
       "         'satisfactori': 1,\n",
       "         'glad': 1,\n",
       "         'met': 1,\n",
       "         'bless': 1,\n",
       "         'firstli': 1,\n",
       "         'moredoctor': 1,\n",
       "         'tonsil': 1,\n",
       "         'temporari': 1,\n",
       "         'relief': 1,\n",
       "         'procedur': 1,\n",
       "         'fraction': 1,\n",
       "         'surgey': 1,\n",
       "         'whether': 1,\n",
       "         'peel': 1,\n",
       "         'identif': 1,\n",
       "         'goodexplan': 1,\n",
       "         'mint': 1,\n",
       "         'h': 1,\n",
       "         'pylori': 1,\n",
       "         'patientin': 1,\n",
       "         'blousesaft': 1,\n",
       "         'wash': 1,\n",
       "         'handsi': 1,\n",
       "         'anywherefor': 1,\n",
       "         'rupeestot': 1,\n",
       "         'kushi': 1,\n",
       "         'registr': 1,\n",
       "         'intent': 1,\n",
       "         'collect': 1,\n",
       "         'laksh': 1,\n",
       "         'u': 1,\n",
       "         'progress': 1,\n",
       "         'zero': 1,\n",
       "         'experiencedlisten': 1,\n",
       "         'problemsexplain': 1,\n",
       "         'treatmentoveral': 1,\n",
       "         'medicinereli': 1,\n",
       "         'part': 1,\n",
       "         'colonoscopi': 1,\n",
       "         'availbl': 1,\n",
       "         'nijampet': 1,\n",
       "         'burri': 1,\n",
       "         'diabet': 1,\n",
       "         'unkind': 1,\n",
       "         'sometim': 1,\n",
       "         'ban': 1,\n",
       "         'remark': 1,\n",
       "         'plan': 1,\n",
       "         'outstand': 1,\n",
       "         'safe': 1,\n",
       "         'matter': 1,\n",
       "         'ur': 1,\n",
       "         'strong': 1,\n",
       "         'bookedso': 1,\n",
       "         'warn': 1,\n",
       "         'degre': 1,\n",
       "         'idea': 1,\n",
       "         'medicalh': 1,\n",
       "         'kill': 1,\n",
       "         'chronic': 1,\n",
       "         'big': 1,\n",
       "         'protect': 1,\n",
       "         'instruct': 1,\n",
       "         'write': 1,\n",
       "         'assist': 1,\n",
       "         'chanc': 1,\n",
       "         'eat': 1,\n",
       "         'physic': 1,\n",
       "         'activ': 1,\n",
       "         'irregular': 1,\n",
       "         'review': 1,\n",
       "         'bodi': 1,\n",
       "         'urgent': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d958f30b-ac31-4c45-ad1b-ca891056b544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86ec93e4-2f8c-4092-85fd-55b4e2f3a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(lines,filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename,'w',encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "save_vocabulary(tokens,\"D:/ML_first_project/static/model/vocabulary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44e5fa15-0698-4207-a369-90590ba19334",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b9b200c-3b99-40ce-a6cf-33650306d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>labels</th>\n",
       "      <th>tag</th>\n",
       "      <th>processed_reviews</th>\n",
       "      <th>binary_representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>he explained initially that it takes 4-5 sitin...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>explain initi take - site total care cost rupe...</td>\n",
       "      <td>{'explain': 1, 'initi': 1, 'take': 1, '-': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>great dr definitely recommend he recommends le...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>great dr definit recommend recommend less medi...</td>\n",
       "      <td>{'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>doctor came and spent 9 seconds and recommende...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>physiotherapist came spent second recommend no...</td>\n",
       "      <td>{'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>i am completely satisfied with the consultatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>complet satisfi telemedicin consult acut sever...</td>\n",
       "      <td>{'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>my experience was nice dr dyed was cool and co...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>experi nice dr dy cool compos patient listen m...</td>\n",
       "      <td>{'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>110</td>\n",
       "      <td>dr is not ready to talk he writes for test and...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>dr readi talk write test ask meet later later ...</td>\n",
       "      <td>{'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>71</td>\n",
       "      <td>doc just spent 3-5 minutes time to review on f...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>doc spent - minut time review full bodi checku...</td>\n",
       "      <td>{'explain': 0, 'initi': 0, 'take': 0, '-': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>28</td>\n",
       "      <td>very friendly and his approach is also very go...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>friendli approach also good clearli explain pr...</td>\n",
       "      <td>{'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>119</td>\n",
       "      <td>overall had a very bad experience and the hosp...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>overal bad experi hospit profession</td>\n",
       "      <td>{'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>117</td>\n",
       "      <td>i have booked appointment and visited the hosp...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>book appoint visit hospit could say worst expe...</td>\n",
       "      <td>{'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            reviews  labels  \\\n",
       "0            93  he explained initially that it takes 4-5 sitin...       0   \n",
       "1            33  great dr definitely recommend he recommends le...       1   \n",
       "2           129  doctor came and spent 9 seconds and recommende...       0   \n",
       "3             7  i am completely satisfied with the consultatio...       1   \n",
       "4            38  my experience was nice dr dyed was cool and co...       1   \n",
       "..          ...                                                ...     ...   \n",
       "138         110  dr is not ready to talk he writes for test and...       0   \n",
       "139          71  doc just spent 3-5 minutes time to review on f...       0   \n",
       "140          28  very friendly and his approach is also very go...       1   \n",
       "141         119  overall had a very bad experience and the hosp...       0   \n",
       "142         117  i have booked appointment and visited the hosp...       0   \n",
       "\n",
       "          tag                                  processed_reviews  \\\n",
       "0    negative  explain initi take - site total care cost rupe...   \n",
       "1    positive  great dr definit recommend recommend less medi...   \n",
       "2    negative  physiotherapist came spent second recommend no...   \n",
       "3    positive  complet satisfi telemedicin consult acut sever...   \n",
       "4    positive  experi nice dr dy cool compos patient listen m...   \n",
       "..        ...                                                ...   \n",
       "138  negative  dr readi talk write test ask meet later later ...   \n",
       "139  negative  doc spent - minut time review full bodi checku...   \n",
       "140  positive  friendli approach also good clearli explain pr...   \n",
       "141  negative                overal bad experi hospit profession   \n",
       "142  negative  book appoint visit hospit could say worst expe...   \n",
       "\n",
       "                                 binary_representation  \n",
       "0    {'explain': 1, 'initi': 1, 'take': 1, '-': 1, ...  \n",
       "1    {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "2    {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "3    {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "4    {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "..                                                 ...  \n",
       "138  {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "139  {'explain': 0, 'initi': 0, 'take': 0, '-': 1, ...  \n",
       "140  {'explain': 1, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "141  {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "142  {'explain': 0, 'initi': 0, 'take': 0, '-': 0, ...  \n",
       "\n",
       "[143 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6e64777-7963-4551-a001-ead1ad63651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset['processed_reviews']\n",
    "y=dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aff7049f-9929-4a0c-9b98-eba45bc71603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      explain initi take - site total care cost rupe...\n",
       "1      great dr definit recommend recommend less medi...\n",
       "2      physiotherapist came spent second recommend no...\n",
       "3      complet satisfi telemedicin consult acut sever...\n",
       "4      experi nice dr dy cool compos patient listen m...\n",
       "                             ...                        \n",
       "138    dr readi talk write test ask meet later later ...\n",
       "139    doc spent - minut time review full bodi checku...\n",
       "140    friendli approach also good clearli explain pr...\n",
       "141                  overal bad experi hospit profession\n",
       "142    book appoint visit hospit could say worst expe...\n",
       "Name: processed_reviews, Length: 143, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b8cb4e-ca44-4093-9063-185403ac1c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "138    0\n",
       "139    0\n",
       "140    1\n",
       "141    0\n",
       "142    0\n",
       "Name: labels, Length: 143, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "550fa4d7-6233-41f2-916e-b1b0a610bd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\ml_first_project\\env\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ml_first_project\\env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6de97869-df60-43c0-8ba1-1021e0454714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train ,x_test, y_train, y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bb601c3-8701-4fa9-8844-288d4d9b750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85d6a412-8d28-456f-857a-ce7d859263c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816acd3-961c-4279-893f-ea96e2a5177f",
   "metadata": {},
   "source": [
    "### vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56a4b76c-fba6-4d8b-a781-634d1dcd56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorizer(ds,vocabulary):\n",
    "    vectorized_lst=[]\n",
    "    for sentence in ds:\n",
    "        sentence_lst= np.zeros(len(vocabulary))\n",
    "        for i in range (len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_lst[i]=1\n",
    "        vectorized_lst.append(sentence_lst)\n",
    "    vectorized_lst_new=np.asarray(vectorized_lst,dtype=np.float32)\n",
    "    return vectorized_lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30135e7d-2459-476d-8a75-f05daf92cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x_train=vectorizer(x_train,tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e322538c-068f-4644-986a-33534e2db4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "084fb419-b5c0-484c-90e8-90584a0cab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in vectorized_x_train[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee9a5dc3-dd8f-4d60-8e54-36b9678e39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x_test=vectorizer(x_test,tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7aaa4036-3c96-4e67-99c5-7a8c9708a27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c14ec89f-539c-458f-a2be-5de75abdf9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    58\n",
       "1    56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f93671a7-5a1d-4d29-ba37-dff1192eb5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm0klEQVR4nO3dB5RV5bnG8Wd6HxhmhiJSpUovigUrKFi52AuKvcVriUZjbowt1lwNV42amERBxYI1AgbFgoIGAWnSe5ehDDC9nru+feAoUpz+7fL/rXXWzJxzmHkZcT/761GhUCgkAAAkRdsuAADgHoQCACCCUAAARBAKAIAIQgEAEEEoAAAiCAUAQAShAACIIBQAABGEAgAgglAAAEQQCgCACEIBABBBKAAAIggFAEAEoQAAiCAUAAARhAIAIIJQAABEEAoAgAhCAQAQQSgAACIIBQBABKEAAIggFGDVF198oaioKO3YseOg72vbtq1GjRrVYHUBQUUooEquuOIK5+JtHvHx8erQoYMefPBBlZeX1+r7HnPMMdq0aZMaNWrkfP3yyy+rcePG+7xvxowZuu6662r1swD8stgqvAdwDB06VC+99JJKSko0ceJE/epXv1JcXJzuueeeGn9PEzDNmzf/xfdlZ2fX+GcAqDpaCqiyhIQE5wLepk0b3XjjjRo8eLD+9a9/KTc3V5dffrkyMjKUnJys0047TcuWLYv8uTVr1uiss85yXk9JSVG3bt2cUPl595H5/Morr9TOnTsjrZL7779/n+6jSy65RBdeeOFetZWVlSkrK0tjxoxxvq6srNSjjz6qdu3aKSkpSb169dLbb7/dgL8twJtoKaDGzMV227ZtTteSCQETEOnp6br77rt1+umna+HChU5LwrQoSktL9eWXXzqhYJ5PTU3db1eSufD/4Q9/0JIlS5zn9ve+Sy+9VOeff77y8/Mjr0+aNEmFhYUaPny487UJhFdffVUvvPCCOnbs6PzsESNGOC2OE044od5/N4BXEQqotlAopE8//dS5EJtWwfvvv69p06Y5F3XjtddeU6tWrZznzcV77dq1Ovfcc9WjRw/n9fbt2x+wK8mMLZgWwsG6lIYMGeKEy3vvvafLLrvMeW7s2LE6++yzlZaW5nRvPfLII5o8ebKOPvroyM+cOnWq/vrXvxIKwEEQCqiy8ePHO3fmpqvGdM+YbpxzzjnHeX7AgAGR92VmZqpz585atGiR8/Utt9zidDd9/PHHTpeTCYiePXvWuI7Y2FhdcMEFTviYUCgoKNAHH3ygN954w3l9+fLlTqvhlFNO2evPmdZKnz59avxzgSBgTAFVdtJJJ2nOnDlOV1FRUZFGjx7t3NX/kmuuuUYrV650LuDz589X//799cwzz9SqFtOFZForOTk5TovEdGWZgXDDdCsZEyZMcOrd8zDdVowrAAdHKKDKTJeNmYraunVr527d6Nq1qzMtdfr06ZH3mXEGMyZw+OGHR54z3Uk33HCD3n33Xd1xxx168cUXD9iFVFFR8Yu1mK4q8z3ffPNNp8VguqnM+IVhfq4ZFDfdVqbenz7MnwFwYHQfoVbMIO6wYcN07bXXOv31pk//t7/9rVq2bOk8b9x2223O2EOnTp2cmUqff/65Eyb7Y2YZmTt90wowM4bMbCbz2B/TfWUGkpcuXep8zz1MDXfeeaduv/12p5tr4MCBzowmM+5hBsJHjhxZT78NwPtoKaDWzNqFfv366cwzz3QGds1AtJlyuufO3dz5mxlIJghMF48Jh+eee+6ALQDTojBTTs1MoSeeeOKgXUimS8gE0LHHHrvXaw899JDuvfdeZxbSnp9rupPMFFUABxYVMv8HAwBASwEA8FOEAgAgglAAAEQQCgCACEIBABBBKAAAIggFAEAEoQAAiCAUAAAR7H0E3ykpr9D63CL9sLNYBSXlKiqrUEFJhQpLy1VUWqHCsgrnY1lFpSpDIVVUmoecz/d8HRcTrUZJccpIjldGSlzk88bJP35MSwxv4wH4CaEAzzEX7Y07irQut1Drt4c/rtte6ASB+Twnr0QNsXlLbHSUEw4/Bka8MpLj1CYzWZ2apalL83S1apJUpe3FAbdg7yO4lrnT/25trlbk5O++8Bdp/Y5CbdpRrPJKb/yzTY6PUcdmaercLFWdm6ers/m8eZqy0xJslwbsF6EAV6isDGnJ5jzNXJOrWau3a8bqXG3YUSS/ykyJd1oTJiD2PLo0T1NyPI132EUowArTpz97nQmAXCcITIsgr7hcQRYXE6U+rTN0Qqds59HtkHS6ntDgCAU0iJy8Ys00AeCEwHYt3LjLM11AtmSlxmtghyyd0Dlbx3XMVlYqXU6of4QC6nVMYML8TRo/b6PmrNvRIIO/fmUaDKblcHzHbB3fKVv92mQ4M6SAukYooE5tySvRxPmbNGHeJs1Ys50gqCepCbE6+rBMJyBO7JStVk32f2QpUF2EAmpte0GpPvp+k8bP3aTpq7aJXqGGZ1oO5/U7VGf0bKF01k+gFggF1MjOwjJNWvCDPpy3Ud+s2Mb4gEskxEbr1G7NdW7fls44REw0A9WoHkIB1VopbLqG/jVno6Yu36qyCv7puFnTtAQN79tSlxzZWm0yU2yXA48gFPCLcnYV65X/rNHY6Wu1raDUdjmowSC1mcU04qg2Gty1Ga0HHBShgAOat36H/jl1lTODiFaBP7RolKiLjmiti45spWbpibbLgQsRCthnZbEZK/j71FWatSbXdjmox32bTuvRQrcO6qAOTdNslwMXIRTgMDuGvjd7g16YskIrtxTYLgcNxPQknd3rEN06uJPaZTHuAEIh8IrLKvT6t2v14pcrtXFnse1yYIkZZxjep6VuHdSRNQ8BRygEVH5JuV6etkovTVvN4DH22n/JrHe4+eSOatk4yXY5sIBQCBjzn3vczPV6YtISbc0vsV0OXCo+JtoZjP7VSR0YkA4YQiFAzMDxAx8u0Lz1O22XAg8thrt0QBvdeOJhnAEREIRCQNYZPPbRYr03ZwN7EaFGkuJidPnRbXT9CYepSUq87XJQjwgFn69A/sfUVfrLZ8tVUFphuxz4QHpirP7njK668IjWtktBPSEUfOrjBT/o4YmLtGZboe1S4EPHHJapR8/pwfYZPkQo+MzynDw98OFCfbVsq+1S4HOJcdG6fXAnXXNce7bO8BFCwSd2FpVp1OSleuWbNexYigbVo2UjPX5uTx1+SLrtUlAHCAWfdBXd8+581hvA6rYZ1x7f3ln8lhgXY7sc1AKh4PHVyH+csFCv/met7VIAR/usFGesYUD7TNuloIYIBY9a8kOebnl9tpZszrNdCrDPVt1mJ9Z7Tu/CKXAeRCh40CvfrNYfJyxSSXml7VKAA2qWnqCHhnV3ToKDdxAKHpJbUKq73pmnTxZutl0KUGVm0du9Zx6uuJho26WgCggFjzDnIN/+5hz9sIudTOE9R7TN0F8u7aumaeyj5HaEgsuVV1Rq1ORleu6L5WKmKbzenfT8iH7q2zrDdik4CELBxdZtL9Stb8zWd2t32C4FqLPdV+87+3Bnkz24E6HgUh/O3ajfvTdfecXltksB6txFR7TSA8O6KSGWNQ1uQyi40JMfL9Ezny23XQZQr3q3aqznR/RVi0Yc5uMmhILLxg/MyuRxs9bbLgVoEFmp8frLJX1Z7OYihIJLFJaW68ZXv9OUpVtslwI0+BYZZjvuK49tZ7sUEAruYI7FvOrlGZyIhkA7p09LPXJOD/ZOsoxQsGzV1gKN/Oe3Wrudcw+AXq0aa/SVR6hxMqe72UIoWDR7ba6uHj1T29ndFIjo3CxNr1xzJAvdLCEULPl00WbdPHa2iso4JhP4uTaZyXr16gFq1STZdimBQyhYMHb6Wt37wfeqYIkycEAtGiXqlasHqEPTVNulBAqh0MCe+niJnmYNAlAlmSnxGn3VkerespHtUgKDUGgglZUh/fbdeXprJmsQgOpIS4x1WgxmsRvqH3vZNgCTu3e/QyAANWG2ernsH9M1dx17gDUEQqEBPPDhQlYpA3UQDPPWEwz1jVCoZ0/8e7Fe/nq17TIAz9tVXK4Rf5+u+SzyrFeEQj36y+fL9dwXK2yXAfgrGP4xXd9vIBjqC6FQT16atkp/mrTEdhmA7+wsKtOlf5+uJT/k2S7FlwiFevD+7A16cPxC22UAvg4Gs1+Y2TcMdYtQqGNTl23Vb96eKyb6AvVrw44iXTdmpkrK2RWgLhEKdWjBxp264dVZKqsgEYCGYI6qvevtebbL8BVCoY6szy3UlS/NUH4Jx2cCDemDORv1zKfLbJfhG4RCHdhRWOpsf52TR/8mYMNTk5dqwrxNtsvwBUKhlkrLK3XtmJlasaXAdilAYJkxvDvGzWFxWx0gFGrpkYmLNGN1ru0ygMArLqvUNaNnatPOItuleBqhUAv//n4Tq5UBFzFduCYYzJnnqBlCoYbWbS/Ub5j1ALjOgo27dPubc5yNKFF9hEINxxF+NfY7Z5MuAO4zacFmPcGOAjVCKNRwHGEem3IBrvb8Fyv0DrsTVxuhUE0fzWccAfCK37//vVZvZWZgdRAK1bB2W6HueodxBMArisoqdMe4uc7Jh6gaQqGKGEcAvGnWmlz99cuVtsvwDEKhih6esFDz2cMd8KQ/T17KVttVRChUwcT5mzT6mzW2ywBQi5a+maZaVlFpuxTXIxR+wZptBbqb9QiA5y3ctEtPs3HeLyIUDsIMTt3y+mzlsfMp4AvmeNw569gf6WAIhYN4dfoazWU9AuAbFZUh/fqtOSou42CeAyEUDsAc8/e/rIgEfGfllgI9/u/FtstwLULhIKuWdzH9FPAlswD16xVbbZfhSoTCfny7arve/W6D7TIA1BOzV95vxs1TXnGZ7VJch1D4mfKKSt37/ve2ywBQzzbsKNJD4xfaLsN1CIX9NCuXbGaRCxAEb81c76x4xo8IhZ/YvKtYoyYzjxkIkkcnLrJdgqsQCj9hmpL5rEkAAmXmmlxNWvCD7TJcg1DYbdryrRo/b5PtMgBYYKaomvFEEAqRfVHu/YDBZSDIaxfemLHOdhmuQChIevGrlc4/CgDB9X+fLlNhKd3HgQ8FMy3t2c+W2y4DgGVb8kr0N85dIBRGfbLUOZ0JAF78cqUTDkEWHfRWwvtzWLkMIKygtEKjJi9VkAU6FP42ZYXKKji7FcCP3pyxTiu25CuoooO8C+qbM5ltAGBv5ZUhPRHgXVQDGwr/mLpKxWXMSwawr0kLNmvWmu0KokCGwq7iMr3KmcsADuKRicFsLQQyFMZ8vZojNgEc1Kw1uYE8cyFwoWCO4Xtp2mrbZQDwgH98tUpBE7hQeP3btdpWUGq7DAAe8NmSHK0M2EykQIVCWUWlszgFAKp6QttLAetZCFQovPfdBm3cWWy7DAAe8s5367WzMDjHdgYmFCorQ3phygrbZQDwmMLSCo39dq2CIjChMPH7TVq5lZ1QAVTfmG9WB+a8hcCEAmMJAGpq085iTV6UoyAIRCgs/mGX5q7fabsMAB42NiBdSIEIhbdnrrddAgCP+2rZFq3bXii/830omH5AtscGUBfTU8cGoLXg+1D4bHGOtuazWA1A7Y2bud5Z7+Rnvg+FcbPoOgJQd1vuf7xgs/zM16GwLb9EXywJxowBAA3jjRn+7kLydSj8a+5GTlYDUKe+WbHN1yucfR0KH87daLsEAD48me1zH/dA+DYUNuwo0ux1O2yXAcCHPlno33EF34bChHkbnSlkAFDXpizdopLyCvmRb0Nh/LxNtksA4FP5JeX6esU2+ZEvQ2HNtgLNY1sLAPXoE592IfkyFGglAKhvkxduVsiHfdS+DIWPF/xguwQAPpeTV+LLjTZ9Fwp5xWX6fuMu22UACIBPFvrvBtR3oTBj9XZVVPqvSQfAfT7x4biC70LhPyu32y4BQEAs3ZzvTGzxEx+Ggj+niQFwp0981lqI9tt4wgLGEwA0oI99tmuqr0Jh5upcxhMANKhZa3O1o9A/Z7b4KhToOgLQ0CoqQ77aZ41QAIBamu+j9QrRftqLhPUJAGyYRyi4z4xVrE8AYMf8DXQfuQ5dRwBs2byrRDl5xfIDQgEA6sB8n3Qh+SIUGE8AYNs8QsE9Zq1hfQIAu+ZvIBRcY9nmPNslAAi4+YSCe6za6q8NqQB4z5a8Em3aWSSv80UorPbZLoUAvGm+D8YV/BEKWwttlwAA8kMXkudDobisQht90GQD4H3zaCnYt2ZboXx4djYAD/qeloJ9DDIDcIttBaXaXuDtbbQ9HwoMMgNwk827vL3dhedDYdUWQgGAe2wmFOxaRUsBgIvk5JXIyzwfCqsZUwDgIjm0FOwpKCn3fCoD8N822l7m6VBg5hEAt8nx+LkKng4FZh4BcJvNtBTs2bTD24kMwH9yGFOwp6C03HYJALCXLfklCnl4mwVPh0JhaYXtEgBgL2UVIU+vavZ4KNBSAOA+mz08ruDtUCihpQDAfXI8PAPJ26FA9xEAF8qhpWAHA80A3CiHloIdRbQUALhQoYevTZ4OhQIP/+IB+Fd5JVNSrSii+wiAC5VXEApW0FIA4EYVlZXyKk+HQmEJLQUA7lNG95EdRWW0FAC4TwXdR3ZmHnk4jAH4WLmHL06x8ii2uMD+xEWH1Dy+RE3jy9Q0oUSZsSVqEleijOgSNYopVqPoYqWpUCkqVHKoSEmVBUqoLFR8RYGiK/k3hbpRmnqqpF7yIs+GAvwlKabCuZA3SyhVdlypsuJK1CS2RI13X8jTo4uUpiKlqEjJoQIlVRYqobJA8eUFii3PV0xZvqJL8xVVVhj+hmY/Mu/uSQaPS2rVW17l2VBIio+xXQIkpcRUqlmCuTM3F/LwxTwztjh8MY8pVnpU8e6LeaGS9tyZV+TvvpgXKKYsT1HmYl5eLJkWt1kI6t3FoEBYtHevT94NhTjv/tLdID22XM3NXbm5mMeXhrtZzCO6SOm7L+apTjdLkZJCheE784oCxZXnK7YsfGceVZqnqIpSycy+42IO/Cjas5dW74ZCVFSUEmKjVVLu3fnANdEkzvSVl6lZfIky48IX84zd3SyNTTdLVKFSTReL081SqMSKAqe/PM48nC6W3XfmlWWSmbxVtPsBoO4QCva6kLwSClm7+8ubxpVE7swzYsIXc9Nf7tyZRxUqZfddeeLuwc89d+bRzp25uZiXS2Y8lDFRwL1i4uRVng6FxFjThVRWb98/KioUnsUSH+5mMY/MmGJlmD5zZ/CzWGm7u1mcO/PKgh/vzE1/uRkALc2TzMU8VBku1Tx2j4UC8Klo73ZvezoUDjTYHBNlBj9NF8vuC7kzk6VUTZyLefiuPC0qPJPFuTMPhfvL97ozdy7mBYoyo597LuYFDf5XBOBFsYnyKk+HwiutxysldaHiy/PDd+Z7LuZlheGLOdMSAdiQki2v8nQoHFq0VPphmu0yAGBvac3lVZ7d5sKRlGG7AgDYVyqhYEdyE9sVAMC+0prJq7wdCrQUALhRKqFgB6EAwI3XpdgEeZXHQ4HuIwAuk+rd8QTvh0JKlu0KAMA34wneD4Um7W1XAAB7o6VgUUZbT288BcCH0mgp2N10ygQDALhFKi0Fu7I62a4AAH5ES8GyzA62KwCAH9FSsCyro+0KAMAX+x75JBToPgLgEnEpUkY7eZn3QyGTlgIAl2jeXYr29mXV29UbKZlsdwHAHVr0ltd5PxQMWgsA3KBFL3mdP0KBcQUAbtCCUHCHLKalAnDBuczZXeR1/ggFuo8A2NasmxTj/W13/BEK2Z1tVwAg6Fp4v+vIP6FgVjUnZ9quAkCQtSAU3CMqSmo70HYVAIKsBaHgLm2Ps10BgKCKiZeadpMf+CcU2h1vuwIAQZXdRYqNlx/4JxTMYHOqt7esBeBRLfzRdeSvUDDoQgJgwyF95Bf+CoV2hAIACzoMll/4KxRoKQBoaNldpYw28gt/hULmYVJ6S9tVAAiSTkPkJ/4KBYPWAoCG1Pk0+Yn/QoFxBQANJamJdOiR8hP/hQItBQANpeOpnj9p7ef89bcxzIBP49a2qwAQBJ38NZ7gz1Aw2p9kuwIAfhcdJ3UYJL/xZyh0P9d2BQD8rs3RUmIj+Y0/Q8GMKzA1FUB96jRUfuTPUDADP7QWANSnToSCt/S80HYFAPwqs2N4sawP+TcUmnf3zf7mAFymk/9mHfk/FIyeF9iuAIAfdRsuv/J3KPQ4X4ry918RQANr3kM6tL/8yt9XzEYtpTbH2q4CgJ/0u1J+5u9QMHpdZLsCAH4Rn+r7bmn/h0LXs6XYJNtVAPBLl3RCmvzM/6GQmC519ud8YgANrP9V8jv/h4LBmgUAtdWyv9Sip/wuOjDnpyZn2q4CgJf1938rITihEBMn9b/adhUAvCqxsdT9HAVBMELBGHCDFJdsuwoAXtTrYikuGBNWghMKKZlS35G2qwDgRf2D0XUUrFAwjrk5fDAGAFRnK/7sTgqKYIVCo0N9v/AEQB3r7+8VzMEOBePY29gPCUDVpLeUupylIAne1dE0A7ucYbsKAF5wwl1SbLyCJHihYAz8te0KALhdk/ZS7xEKmmCGQsu+UvsTbVcBwM1O/J0UE6ugCWYoGLQWABxIs+5Sj/MURMENhfYnSC372a4CgBud9D9SVJSCKLihYAy83XYFANzm0COkLqcrqIIdCl3OlLK72q4CgJucfK+CLNihYJqHQx+xXQUAt2h3fLhrOcCCHQrGYSeHT2cDgEH3KegIBWPII+ygCgRd59OlQ/sr6AgFo3ErpqgCQWa2vjn597arcAVCYY9jbwmvYAQQPN3PlZp1s12FKxAKe8QmSEMft10FgIaWkC6d8qDtKlyDUPipTqdKnYbargJAQzKBkH6I7Spcg1D4uaGPSbGJtqsA0FAH6PS7wnYVrkIo/FyTdtIxt9iuAkB9MzMOz34msNtZHAihsD/H/Vpq1Np2FQDqk5ltZG4CsRdCYX/ikljpDPh9f6MBN9quwpUIhQPpepZ02CDbVQCoazHx0tnPStFc/vaH38rBmP7GpAzbVQCoS8ffJTXtYrsK1yIUDqZRS2nYc7arAFBXmvWQBt5muwpXIxR+idlX/cjrbVcBoLaiYqRhz0gxcbYrcTVCoSpOfUhq3sN2FQBq45j/lg7pY7sK1yMUqroFxnkvS/GptisBUBOZHaUT77FdhScQClWV1UE6/U+2qwBQXXEp0gWjpTh2KqgKQqE6el8i9bzIdhUAqmPYs+yAWg2EQnWd8aTU5DDbVQCoimNvlbqfY7sKTyEUqishVTrvn+EFMADcfdTuoPttV+E5hEJNHNJbGvyA7SoAHEhG2/DNG6uWq43fWE0dfZPU6TTbVQDY3+6nF77GbgQ1RCjUxn89xxGegBsHlpt3t12FZxEKtZHcRBrxrpSSbbsSAIY5C8Wct4waIxRqy+zHfuk4FrYBtrU/SRrMwHJtEQp1wSydN4tjotlTBbCicZvdA8sxtivxPEKhrnQYHN5qG0DDDyxfNDbcnYtaIxTqUu+LpUF/sF0FEBzRsdJ5LzGwXIcIhbp23B3SEdfargLwv6hoafhfpc5DbVfiK4RCfTjtCanr2barAPztzD9LPc6zXYXvEAr1wayiPOdFqfUxtisB/OnUP0r9rrBdhS8RCvXFbNN78etSdlfblQD+O2PZHJiDekEo1KekxtKId6T0lrYrAfzh6Julk//HdhW+RijUt0YtpZEfSo1a2a4E8H4gDHnYdhW+FxUKhUK2iwiEHeukMWdL21fargTw5vYV5qx01DtCoSHlbZbGDJO2LLJdCeCtg3JOedB2FYFB91FDSmsmXTlRatHbdiWANwy8nUBoYLQUbCjeJY29QFr7je1KAPc68R7pxN/ariJwCAVbyoqkt6+Slky0XQngLjEJ0rC/SD3Pt11JIBEKNlVWSBN+Lc162XYlgDuYs0nM5natjrRdSWARCm4w5Qnpc6baIeDMQs9L3pQy2tiuJNAIBbf47hVp/G1SZbntSgA7W8+b3U4T021XEniEgpss+0Qad4VUmm+7EqDhHHmdNPQxDshxCULBbbYuk94aKeUssF0JUL+iYsJhMOA625XgJwgFt85MmvgbafYrtisB6kdCeri7qONg25XgZwgFN5vzenh2Ulmh7UqAutO4tXTJW1JTdhB2I0LB7XIWS+NGSlsW264EqL02x0rnj5ZSs21XggMgFLygtECacIc093XblQA1ExMvnfx76ej/Dh9CBdciFLzkuzHSxLuk8iLblQBV16x7+Czl5t1tV4IqIBS8ZvMC6a3LpW3LbVcCHFxUdPiEtJN+L8XG264GVUQoeFFJvvThrdL3b9uuBNi/xm2k4S9IbTin3GsIBa93J318r1S8w3YlwI/6XCYNfVRKSLNdCWqAUPC6/C3SJ/cyCA13bGZ31tNSl9NtV4JaIBT8YvVUafyvpa1LbFeCIOpypnTW/0kpWbYrQS0RCn5SUSZ9/Yz05Z9Y8IaGkdhYGvKw1GeE7UpQRwgFP8pdI310t7T0I9uVwK+i46Qjr5WO/42U3MR2NahDhIKfLRofDodd621XAj/pepY0+AEp8zDblaAeEApBWA39xWPSf57jrAbUziF9w11FTDP1NUIhKHIWhbfKWDPNdiXwmkatpEH3ST3Ok6KibFeDekYoBM2Kz8MD0YQDqrK99cDbpaNukuISbVeDBkIoBNXqadKXT0grv7BdCdwmOlbqd4V04j1MMQ0gQiHo1s0Ih8Oyj21XAjfsVdTlDOnke6XszrargSWEAsI2zpa+/F9p8QRJ/JMIlPi08DqDAddLTdrZrgaWEQrYdxdWM+aw8AMpVGm7GtT3pnUmCMxeRYnptquBSxAK2L8tS6WvnpTmj5NCFbarQV1qfYx09E1S5zM48Ab7IBRwcDvXh8+KnjtW2r7SdjWozcln3c6RjrpROqS37WrgYoQCqm7tf6Q5r0kL3pdKdtmuBlWRnCn1v0o64hoprbntauABhAKqr6xIWvRhOCBWfcnYgxtbBe1PkroNDz9YY4BqIBRQ++4lc5aD6WLavsJ2NcEVmygdNkg6fJjUeaiU2Mh2RfAoQgF1Z+303d1L79G91BBik6SOg6XD/0vqNFRKSLVdEXyAUEDdKy8Jjz+s/Dy8rcYP8+hiqitxKVKnU8Mtgo6nSvEptiuCzxAKqH+F26VVU8IBYYJix1rbFXlLajOp3fHhIOgwWIpLsl0RfIxQQMMzU1v3BMSqr6TiHbYrco+oGKlZN6nVkVKrAeGPGW1tV4UAIRRgV2WFtHGOtPKz8CZ9OQul/M0KjKQM6dAjwhf/Q4+UWvZjbABWEQpwZ3fTliXSlkVSzuIfPxbkyPNTRZscJh3af3crYICU1ZEzCuAqhAK8FRbmsKBIWCwOf124Va656Ke3lBq33v1o8+PnGW2k1OZsKwHXIxTgfUU7pMJt4Y9FuVV7mHGMgx1Pavr2zdz/+GQpLlmKT/3J5ynhw+ojF/3dH9NacNGH5xEKCK6S/PBU2eiYcAhEPnJhR3ARCgCACG6JAAARhAIAIIJQAABEEAoAgAhCAQAQQSgAACIIBcAH2rZtq1GjRtkuAz5AKAC/4IorrlBUVJQee+yxvZ5///33necb0ssvv6zGjRvv8/yMGTN03XXXNWgt8CdCAaiCxMREPf7448rNzZUbZWdnKzk52XYZ8AFCAaiCwYMHq3nz5nr00UcP+J6pU6fquOOOU1JSklq1aqVbbrlFBQUFkdc3bdqkM844w3m9Xbt2Gjt27D7dPk899ZR69OihlJQU53vcdNNNys/Pd1774osvdOWVV2rnzp1OC8U87r//fue1n36fSy65RBdeeOFetZWVlSkrK0tjxoxxvq6srHT+LqYOU0+vXr309ttv1/FvDV5EKABVEBMTo0ceeUTPPPOM1q9fv8/rK1as0NChQ3Xuuedq3rx5evPNN52QuPnmmyPvufzyy7Vx40bn4v7OO+/ob3/7m3Jy9t4OPDo6Wk8//bQWLFig0aNH67PPPtNdd93lvHbMMcc4F/709HQnYMzjzjvv3KeWSy+9VB9++GEkTIxJkyapsLBQw4cPd742gWAC4oUXXnB+1u23364RI0ZoypQpdfp7gweZvY8AHNjIkSNDw4YNcz4/6qijQldddZXz+XvvvWf2DXM+v/rqq0PXXXfdXn/uq6++CkVHR4eKiopCixYtct47Y8aMyOvLli1znvvzn/98wJ89bty4UGZmZuTrl156KdSoUaN93temTZvI9ykrKwtlZWWFxowZE3n94osvDl144YXO58XFxaHk5OTQ119/vdf3MH8H8z4EW6ztUAK8xIwrnHzyyfvcoc+dO9dpIbz22muR58xek6abZtWqVVq6dKliY2PVt2/fyOsdOnRQRkbGXt9n8uTJzl384sWLtWvXLpWXl6u4uNi5y6/qmIH5ORdccIFTy2WXXeZ0YX3wwQd64403nNeXL1/ufL9TTjllrz9XWlqqPn361Oj3Av8gFIBqOP744zVkyBDdc889zqykPUxXzfXXX++MI/xc69atnVD4JatXr9aZZ56pG2+8UQ8//LCaNGnidEFdffXVzgW7OgPJpgvphBNOcLqnPvnkE2fcwHRv7anVmDBhglq2bLnXn0tISKjyz4A/EQpANZmpqb1791bnzp0jz5kWwMKFC527//0x7zV3/bNnz1a/fv0id+w/nc00a9Ysp2Xx5JNPOmMLxltvvbXX94mPj1dFRcUv1mjGH8xAtRnb+Oijj3T++ecrLi7Oee3www93Lv5r1651ggP4KUIBqCYzO8jciZsB4T3uvvtuHXXUUc7A8jXXXOPMHjIhYe7Sn332WXXp0sWZwWTWEjz//PPOBfqOO+5w7uD3rHUwgWJmCZnB7LPOOkvTpk1zBoJ/yswyMnf6n376qTNjyLQeDtSCMLOQzJ83rZTPP/888nxaWprT/WUGl00IDRw40JnRZH6eGcQeOXJkvf3u4AG2BzUALw0077Fq1apQfHx8ZKDZ+Pbbb0OnnHJKKDU1NZSSkhLq2bNn6OGHH468vnHjxtBpp50WSkhIcAaGx44dG2ratGnohRdeiLznqaeeCrVo0SKUlJQUGjJkiDNYbH5Gbm5u5D033HCDM/hsnr/vvvv2GWjeY+HChc57zGuVlZV7vWa+HjVqVKhz586huLi4UHZ2tvPzpkyZUoe/OXgRJ68BlpipraaLxwwuDxo0yHY5gINQABqIWXNgun5M95NZY2DWH2zYsMHp3tnT3w/YxpgC0EDMeMHvfvc7rVy50unXN4PBZtoogQA3oaUAAIhgmwsAQAShAACIIBQAABGEAgAgglAAAEQQCgCACEIBABBBKAAAIggFAEAEoQAAiCAUAAARhAIAIIJQAABEEAoAgAhCAQAQQSgAACIIBQBABKEAAIggFAAAEYQCACCCUAAARBAKAIAIQgEAEEEoAAC0x/8DdTqIk/Uh3cYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(np.array([y_train.value_counts()[0], y_train.value_counts()[1]]), labels=[\"Positive\", \"Negative\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12515000-b833-47b6-970b-c95db01ed1cb",
   "metadata": {},
   "source": [
    "### model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f87365d2-a201-402f-8b5d-3a32083a75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble methods (e.g., Random Forest and AdaBoost)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4939b9c9-e263-4da3-ad60-cef1656db19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "def training_scores(y_act, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate training performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_act: Actual target labels\n",
    "    - y_pred: Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing accuracy, precision, recall, and F1 score\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_act, y_pred)\n",
    "    precision = precision_score(y_act, y_pred, average='binary')  # Change 'binary' if you have multi-class\n",
    "    recall = recall_score(y_act, y_pred, average='binary')  # Change 'binary' if you have multi-class\n",
    "    f1 = f1_score(y_act, y_pred, average='binary')  # Change 'binary' if you have multi-class\n",
    "\n",
    "    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
    "def validation_scores(y_val_act, y_val_pred):\n",
    "    \"\"\"\n",
    "    Calculate validation performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_val_act: Actual target labels for the validation set\n",
    "    - y_val_pred: Predicted labels for the validation set\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing accuracy, precision, recall, and F1 score\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_val_act, y_val_pred)\n",
    "    precision = precision_score(y_val_act, y_val_pred, average='binary')  # Adjust based on your class type\n",
    "    recall = recall_score(y_val_act, y_val_pred, average='binary')\n",
    "    f1 = f1_score(y_val_act, y_val_pred, average='binary')\n",
    "\n",
    "    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba881c08-783e-43eb-a998-ec3bc5f875c9",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab775870-f244-41c3-9d3d-795540df20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.9824561403508771, 'Precision': 0.9821428571428571, 'Recall': 0.9821428571428571, 'F1 Score': 0.9821428571428571}\n",
      "{'Accuracy': 0.7241379310344828, 'Precision': 0.8888888888888888, 'Recall': 0.5333333333333333, 'F1 Score': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression()\n",
    "lr.fit(vectorized_x_train,y_train)\n",
    "y_train_pred=lr.predict(vectorized_x_train)\n",
    "y_test_pred=lr.predict(vectorized_x_test)\n",
    "print(training_scores(y_train, y_train_pred))\n",
    "print(validation_scores(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fdc3994-5e3f-4278-b2e6-ab1ae2467a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Scores:\n",
      "{'Accuracy': 0.9298245614035088, 'Precision': 0.8870967741935484, 'Recall': 0.9821428571428571, 'F1 Score': 0.9322033898305084}\n",
      "Naive Bayes Validation Scores:\n",
      "{'Accuracy': 0.7931034482758621, 'Precision': 0.8461538461538461, 'Recall': 0.7333333333333333, 'F1 Score': 0.7857142857142857}\n",
      "Decision Tree Training Scores:\n",
      "{'Accuracy': 0.9912280701754386, 'Precision': 1.0, 'Recall': 0.9821428571428571, 'F1 Score': 0.990990990990991}\n",
      "Decision Tree Validation Scores:\n",
      "{'Accuracy': 0.6206896551724138, 'Precision': 0.6428571428571429, 'Recall': 0.6, 'F1 Score': 0.6206896551724138}\n",
      "Random Forest Training Scores:\n",
      "{'Accuracy': 0.9912280701754386, 'Precision': 0.9824561403508771, 'Recall': 1.0, 'F1 Score': 0.9911504424778761}\n",
      "Random Forest Validation Scores:\n",
      "{'Accuracy': 0.7241379310344828, 'Precision': 0.8888888888888888, 'Recall': 0.5333333333333333, 'F1 Score': 0.6666666666666666}\n",
      "AdaBoost Training Scores:\n",
      "{'Accuracy': 0.9473684210526315, 'Precision': 0.9629629629629629, 'Recall': 0.9285714285714286, 'F1 Score': 0.9454545454545454}\n",
      "AdaBoost Validation Scores:\n",
      "{'Accuracy': 0.6206896551724138, 'Precision': 0.75, 'Recall': 0.4, 'F1 Score': 0.5217391304347826}\n",
      "SVM Training Scores:\n",
      "{'Accuracy': 0.9736842105263158, 'Precision': 0.9649122807017544, 'Recall': 0.9821428571428571, 'F1 Score': 0.9734513274336283}\n",
      "SVM Validation Scores:\n",
      "{'Accuracy': 0.7241379310344828, 'Precision': 0.8888888888888888, 'Recall': 0.5333333333333333, 'F1 Score': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(vectorized_x_train, y_train)\n",
    "y_train_pred_nb = nb.predict(vectorized_x_train)\n",
    "y_test_pred_nb = nb.predict(vectorized_x_test)\n",
    "\n",
    "print(\"Naive Bayes Training Scores:\")\n",
    "print(training_scores(y_train, y_train_pred_nb))\n",
    "print(\"Naive Bayes Validation Scores:\")\n",
    "print(validation_scores(y_test, y_test_pred_nb))\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(vectorized_x_train, y_train)\n",
    "y_train_pred_dt = dt.predict(vectorized_x_train)\n",
    "y_test_pred_dt = dt.predict(vectorized_x_test)\n",
    "\n",
    "print(\"Decision Tree Training Scores:\")\n",
    "print(training_scores(y_train, y_train_pred_dt))\n",
    "print(\"Decision Tree Validation Scores:\")\n",
    "print(validation_scores(y_test, y_test_pred_dt))\n",
    "\n",
    "\n",
    "# Random Forest Classifier (Ensemble Method)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(vectorized_x_train, y_train)\n",
    "y_train_pred_rf = rf.predict(vectorized_x_train)\n",
    "y_test_pred_rf = rf.predict(vectorized_x_test)\n",
    "\n",
    "print(\"Random Forest Training Scores:\")\n",
    "print(training_scores(y_train, y_train_pred_rf))\n",
    "print(\"Random Forest Validation Scores:\")\n",
    "print(validation_scores(y_test, y_test_pred_rf))\n",
    "\n",
    "\n",
    "# AdaBoost Classifier (Ensemble Method)\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(vectorized_x_train, y_train)\n",
    "y_train_pred_ab = ab.predict(vectorized_x_train)\n",
    "y_test_pred_ab = ab.predict(vectorized_x_test)\n",
    "\n",
    "print(\"AdaBoost Training Scores:\")\n",
    "print(training_scores(y_train, y_train_pred_ab))\n",
    "print(\"AdaBoost Validation Scores:\")\n",
    "print(validation_scores(y_test, y_test_pred_ab))\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(vectorized_x_train, y_train)\n",
    "y_train_pred_svm = svm.predict(vectorized_x_train)\n",
    "y_test_pred_svm = svm.predict(vectorized_x_test)\n",
    "\n",
    "print(\"SVM Training Scores:\")\n",
    "print(training_scores(y_train, y_train_pred_svm))\n",
    "print(\"SVM Validation Scores:\")\n",
    "print(validation_scores(y_test, y_test_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56eaf8d6-0950-4ac9-b4fa-97d3c3700cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Corrected file path and mode\n",
    "with open('D:/ML_first_project/static/model/model.pickle', 'wb') as file:\n",
    "    pickle.dump(nb, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2299e2-9c29-40a6-8dbe-335e124aebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
